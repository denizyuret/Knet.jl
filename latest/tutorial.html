<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction to Knet · Knet.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Knet.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="install.html">Setting up Knet</a></li><li class="current"><a class="toctext" href="tutorial.html">Introduction to Knet</a><ul class="internal"><li><a class="toctext" href="#Philosophy-1">Philosophy</a></li><li><a class="toctext" href="#Tutorial-1">Tutorial</a></li><li><a class="toctext" href="#Benchmarks-1">Benchmarks</a></li><li><a class="toctext" href="#Under-the-hood-1">Under the hood</a></li><li><a class="toctext" href="#Contributing-1">Contributing</a></li></ul></li><li><a class="toctext" href="reference.html">Reference</a></li></ul></li><li><span class="toctext">Textbook</span><ul><li><a class="toctext" href="backprop.html">Backpropagation</a></li><li><a class="toctext" href="softmax.html">Softmax Classification</a></li><li><a class="toctext" href="mlp.html">Multilayer Perceptrons</a></li><li><a class="toctext" href="cnn.html">Convolutional Neural Networks</a></li><li><a class="toctext" href="rnn.html">Recurrent Neural Networks</a></li><li><a class="toctext" href="rl.html">Reinforcement Learning</a></li><li><a class="toctext" href="opt.html">Optimization</a></li><li><a class="toctext" href="gen.html">Generalization</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="tutorial.html">Introduction to Knet</a></li></ul><a class="edit-page" href="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/tutorial.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Introduction to Knet</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Introduction-to-Knet-1" href="#Introduction-to-Knet-1">Introduction to Knet</a></h1><p><a href="https://denizyuret.github.io/Knet.jl/latest"><img src="https://img.shields.io/badge/docs-latest-blue.svg" alt/></a> <a href="https://travis-ci.org/denizyuret/Knet.jl"><img src="https://travis-ci.org/denizyuret/Knet.jl.svg?branch=master" alt/></a> <a href="http://pkg.julialang.org/?pkg=Knet"><img src="http://pkg.julialang.org/badges/Knet_0.6.svg" alt/></a> <a href="http://ci.maleadt.net/shields/url.php?builder=Knet-julia06-x86-64bit"><img src="http://ci.maleadt.net/shields/build.php?builder=Knet-julia06-x86-64bit&amp;name=julia%200.6" alt/></a> <a href="http://ci.maleadt.net/shields/url.php?builder=Knet-juliadev-x86-64bit"><img src="http://ci.maleadt.net/shields/build.php?builder=Knet-juliadev-x86-64bit&amp;name=julia%20dev" alt/></a></p><p><a href="https://denizyuret.github.io/Knet.jl/latest">Knet</a> (pronounced &quot;kay-net&quot;) is the <a href="http://www.ku.edu.tr/en">Koç University</a> deep learning framework implemented in <a href="http://docs.julialang.org">Julia</a> by <a href="http://www.denizyuret.com">Deniz Yuret</a> and collaborators.  It supports GPU operation and automatic differentiation using dynamic computational graphs for models defined in plain Julia.  This document is a tutorial introduction to Knet.  Check out the <a href="https://denizyuret.github.io/Knet.jl/latest">full documentation</a> and <a href="https://github.com/denizyuret/Knet.jl/tree/master/examples">Examples</a> for more information. If you need help or would like to request a feature, please consider joining the <a href="https://groups.google.com/forum/#!forum/knet-users">knet-users</a> mailing list. If you find a bug, please open a <a href="https://github.com/denizyuret/Knet.jl/issues">GitHub issue</a>. If you would like to contribute to Knet development, check out the <a href="https://groups.google.com/forum/#!forum/knet-dev">knet-dev</a> mailing list and <a href="install.html#Tips-for-developers-1">Tips for developers</a>. If you use Knet in academic work, <a href="https://goo.gl/zeUBFr">here is a paper</a> that can be cited:</p><pre><code class="language-none">@inproceedings{knet2016mlsys,
  author={Yuret, Deniz},
  title={Knet: beginning deep learning with 100 lines of Julia},
  year={2016},
  booktitle={Machine Learning Systems Workshop at NIPS 2016}
}</code></pre><p><strong>Contents</strong></p><ul><li><a href="tutorial.html#Introduction-to-Knet-1">Introduction to Knet</a></li><ul><li><a href="tutorial.html#Philosophy-1">Philosophy</a></li><li><a href="tutorial.html#Tutorial-1">Tutorial</a></li><ul><li><a href="tutorial.html#Linear-regression-1">Linear regression</a></li><li><a href="tutorial.html#Softmax-classification-1">Softmax classification</a></li><li><a href="tutorial.html#Multi-layer-perceptron-1">Multi-layer perceptron</a></li><li><a href="tutorial.html#Convolutional-neural-network-1">Convolutional neural network</a></li><li><a href="tutorial.html#Recurrent-neural-network-1">Recurrent neural network</a></li></ul><li><a href="tutorial.html#Benchmarks-1">Benchmarks</a></li><li><a href="tutorial.html#Under-the-hood-1">Under the hood</a></li><ul><li><a href="tutorial.html#KnetArrays-1">KnetArrays</a></li><li><a href="tutorial.html#AutoGrad-1">AutoGrad</a></li></ul><li><a href="tutorial.html#Contributing-1">Contributing</a></li></ul></ul><h2><a class="nav-anchor" id="Philosophy-1" href="#Philosophy-1">Philosophy</a></h2><p>Knet uses dynamic computational graphs generated at runtime for automatic differentiation of (almost) any Julia code.  This allows machine learning models to be implemented by defining just the forward calculation (i.e. the computation from parameters and data to loss) using the full power and expressivity of Julia. The implementation can use helper functions, loops, conditionals, recursion, closures, tuples and dictionaries, array indexing, concatenation and other high level language features, some of which are often missing in the restricted modeling languages of static computational graph systems like Theano, Torch, Caffe and Tensorflow.  GPU operation is supported by simply using the KnetArray type instead of regular Array for parameters and data.</p><p>Knet builds a dynamic computational graph by recording primitive operations during forward calculation.  Only pointers to inputs and outputs are recorded for efficiency.  Therefore array overwriting is not supported during forward and backward passes.  This encourages a clean functional programming style.  High performance is achieved using custom memory management and efficient GPU kernels.  See <a href="tutorial.html#Under-the-hood-1">Under the hood</a> for more details.</p><h2><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h2><p>In Knet, a machine learning model is defined using plain Julia code. A typical model consists of a <strong>prediction</strong> and a <strong>loss</strong> function. The prediction function takes model parameters and some input, returns the prediction of the model for that input. The loss function measures how bad the prediction is with respect to some desired output. We train a model by adjusting its parameters to reduce the loss. In this section we will see the prediction, loss, and training functions for five models: linear regression, softmax classification, fully-connected, convolutional and recurrent neural networks.  It would be best to copy paste and modify these examples on your own computer.  They are also available as an <a href="https://github.com/denizyuret/Knet.jl/tree/master/examples/knet-tutorial">IJulia notebook</a> You can install Knet using <code>Pkg.add(&quot;Knet&quot;)</code> in Julia.</p><h3><a class="nav-anchor" id="Linear-regression-1" href="#Linear-regression-1">Linear regression</a></h3><p>Here is the prediction function and the corresponding quadratic loss function for a simple linear regression model:</p><pre><code class="language-julia">using Knet

predict(w,x) = w[1]*x .+ w[2]

loss(w,x,y) = mean(abs2,y-predict(w,x))</code></pre><p>The variable <code>w</code> is a list of parameters (it could be a Tuple, Array, or Dict), <code>x</code> is the input and <code>y</code> is the desired output. To train this model, we want to adjust its parameters to reduce the loss on given training examples. The direction in the parameter space in which the loss reduction is maximum is given by the negative gradient of the loss. Knet uses the higher-order function <a href="reference.html#AutoGrad.grad"><code>grad</code></a> from <a href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad.jl</a> to compute the gradient direction:</p><pre><code class="language-julia">lossgradient = grad(loss)</code></pre><p>Note that <code>grad</code> is a higher-order function that takes and returns other functions. The <code>lossgradient</code> function takes the same arguments as <code>loss</code>, e.g. <code>dw = lossgradient(w,x,y)</code>. Instead of returning a loss value, <code>lossgradient</code> returns <code>dw</code>, the gradient of the loss with respect to its first argument <code>w</code>. The type and size of <code>dw</code> is identical to <code>w</code>, each entry in <code>dw</code> gives the derivative of the loss with respect to the corresponding entry in <code>w</code>. </p><p>Given some training <code>data = [(x1,y1),(x2,y2),...]</code>, here is how we can train this model:</p><pre><code class="language-julia">function train(w, data; lr=.1)
    for (x,y) in data
        dw = lossgradient(w, x, y)
	for i in 1:length(w)
	    w[i] -= lr * dw[i]
	end	    
    end
    return w
end</code></pre><p>We simply iterate over the input-output pairs in data, calculate the lossgradient for each example, and move the parameters in the negative gradient direction with a step size determined by the learning rate <code>lr</code>.  </p><blockquote><p><a href="https://archive.ics.uci.edu/ml/datasets/Housing"><img src="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/images/housing.jpeg?raw=true" alt="image"/></a></p></blockquote><p>Let&#39;s train this model on the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing">Boston Housing</a> dataset from the UCI Machine Learning Repository.</p><pre><code class="language-julia">include(Knet.dir(&quot;data&quot;,&quot;housing.jl&quot;))
x,y = housing()
w = Any[ 0.1*randn(1,13), 0.0 ]
for i=1:10; train(w, [(x,y)]); println(loss(w,x,y)); end
# 366.0463078055053
# ...
# 29.63709385230451</code></pre><p>The dataset has housing related information for 506 neighborhoods in Boston from 1978. Each neighborhood is represented using 13 attributes such as crime rate or distance to employment centers. The goal is to predict the median value of the houses given in $1000&#39;s. The <code>housing()</code> function from <code>housing.jl</code> downloads, splits and normalizes the data.  We initialize the parameters randomly and take 10 steps in the negative gradient direction. We can see the loss dropping from 366.0 to 29.6. See the <a href="https://github.com/denizyuret/Knet.jl/blob/master/examples/housing-linreg">housing</a> example for more information on this model.</p><p>Note that <code>grad</code> was the only function used that is not in the Julia standard library. This is typical of models defined in Knet, where most of the code is written in plain Julia.</p><h3><a class="nav-anchor" id="Softmax-classification-1" href="#Softmax-classification-1">Softmax classification</a></h3><p>In this example we build a simple classification model for the <a href="http://yann.lecun.com/exdb/mnist">MNIST</a> handwritten digit recognition dataset. MNIST has 60000 training and 10000 test examples. Each input x consists of 784 pixels representing a 28x28 image. The corresponding output indicates the identity of the digit 0..9.</p><blockquote><p><a href="https://jamesmccaffrey.wordpress.com/2014/06/10/working-with-the-mnist-image-recognition-data-set"><img src="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/images/firsteightimages.jpg?raw=true" alt="image"/></a></p></blockquote><p>(<a href="https://jamesmccaffrey.wordpress.com/2014/06/10/working-with-the-mnist-image-recognition-data-set">image source</a>)</p><p>Classification models handle discrete outputs, as opposed to regression models which handle numeric outputs. We typically use the cross entropy loss function in classification models:</p><pre><code class="language-julia">predict(w,x) = w[1]*mat(x) .+ w[2]

loss(w,x,ygold) = nll(predict(w,x), ygold)

lossgradient = grad(loss)</code></pre><p><a href="reference.html#Knet.nll"><code>nll</code></a> computes the negative log likelihood of your predictions compared to the correct answers.  Here, we assume <code>ygold</code> is an array of N integers indicating the correct answers for N instances (we use ygold=10 to represent the 0 answer) and <code>predict()</code> gives us a (10,N) matrix of scores for each answer. <a href="reference.html#Knet.mat"><code>mat</code></a> is needed to convert the (28,28,1,N) x array to a (784,N) matrix so it can be used in matrix multiplication.  Other than the change of loss function, the softmax model is identical to the linear regression model. We use the same <code>predict</code> (except for <code>mat</code> reshaping), <code>train</code> and set <code>lossgradient=grad(loss)</code> as before.</p><p>Now let&#39;s train a model on the MNIST data:</p><pre><code class="language-julia">include(Knet.dir(&quot;data&quot;,&quot;mnist.jl&quot;))
xtrn, ytrn, xtst, ytst = mnist()
dtrn = minibatch(xtrn, ytrn, 100)
dtst = minibatch(xtst, ytst, 100)
w = Any[ 0.1f0*randn(Float32,10,784), zeros(Float32,10,1) ]
println((:epoch, 0, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))
for epoch=1:10
    train(w, dtrn; lr=0.5)
    println((:epoch, epoch, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))
end

# (:epoch,0,:trn,0.11761667f0,:tst,0.121f0)
# (:epoch,1,:trn,0.9005f0,:tst,0.9048f0)
# ...
# (:epoch,10,:trn,0.9196f0,:tst,0.9153f0)</code></pre><p>Calling <code>mnist()</code> from <code>mnist.jl</code> loads the MNIST data, downloading it from the internet if necessary, and provides a training set (xtrn,ytrn) and a test set (xtst,ytst). <a href="reference.html#Knet.minibatch"><code>minibatch</code></a> is used to rearrange the data into chunks of 100 instances. After randomly initializing the parameters we train for 10 epochs, printing out training and test set <a href="reference.html#Knet.accuracy"><code>accuracy</code></a> at every epoch. The final accuracy of about 92% is close to the limit of what we can achieve with this type of model. To improve further we must look beyond linear models.</p><h3><a class="nav-anchor" id="Multi-layer-perceptron-1" href="#Multi-layer-perceptron-1">Multi-layer perceptron</a></h3><p>A multi-layer perceptron, i.e. a fully connected feed-forward neural network, is basically a bunch of linear regression models stuck together with non-linearities in between.</p><blockquote><p><a href="http://cs231n.github.io/neural-networks-1"><img src="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/images/neural_net2.jpeg?raw=true" alt="image"/></a></p></blockquote><p>(<a href="http://cs231n.github.io/neural-networks-1">image source</a>)</p><p>We can define a MLP by slightly modifying the predict function:</p><pre><code class="language-julia">function predict(w,x)
    x = mat(x)
    for i=1:2:length(w)-2
        x = relu.(w[i]*x .+ w[i+1])
    end
    return w[end-1]*x .+ w[end]
end</code></pre><p>Here <code>w[2k-1]</code> is the weight matrix and <code>w[2k]</code> is the bias vector for the k&#39;th layer. <a href="reference.html#Knet.relu"><code>relu</code></a> implements the popular rectifier non-linearity: <code>relu.(x) = max.(0,x)</code>.  Note that if w only has two entries, this is equivalent to the linear and softmax models. By adding more entries to w, we can define multi-layer perceptrons of arbitrary depth. Let&#39;s define one with a single hidden layer of 64 units:</p><pre><code class="language-julia">w = Any[ 0.1f0*randn(Float32,64,784), zeros(Float32,64,1),
         0.1f0*randn(Float32,10,64),  zeros(Float32,10,1) ]</code></pre><p>The rest of the code is the same as the softmax model. We can use the same cross-entropy loss function and the same training script. To introduce alternative optimizers, let us use a different train function:</p><pre><code class="language-julia">function train(model, data, optim)
    for (x,y) in data
        grads = lossgradient(model,x,y)
        update!(model, grads, optim)
    end
end</code></pre><p>Here the <code>optim</code> argument specifies the optimization algorithm and state for each model parameter, see <a href="reference.html#Optimization-methods-1">Optimization methods</a> for available algorithms.  <a href="reference.html#Knet.update!"><code>update!</code></a> uses <code>optim</code> to update each model parameter and optimization state.  <code>optim</code> has the same size and shape as <code>model</code>, i.e. we have a separate optimizer for each model parameter. For simplicity we will use the <a href="reference.html#Knet.optimizers"><code>optimizers</code></a> function to use the <a href="reference.html#Knet.Adam"><code>Adam</code></a> optimizer with all parameters:</p><pre><code class="language-julia">o = optimizers(w, Adam)
println((:epoch, 0, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))
for epoch=1:10
    train(w, dtrn, o)
    println((:epoch, epoch, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))
end</code></pre><p>The code for this example is available in the <a href="https://github.com/denizyuret/Knet.jl/blob/master/examples/mnist-mlp">mnist-mlp</a> example or the <a href="https://github.com/denizyuret/Knet.jl/blob/master/examples/knet-tutorial">knet-tutorial</a> notebook.  The multi-layer perceptron does significantly better than the softmax model:</p><pre><code class="language-julia">(:epoch,0,:trn,0.10166667f0,:tst,0.0977f0)
(:epoch,1,:trn,0.9389167f0,:tst,0.9407f0)
...
(:epoch,10,:trn,0.9866f0,:tst,0.9735f0)</code></pre><h3><a class="nav-anchor" id="Convolutional-neural-network-1" href="#Convolutional-neural-network-1">Convolutional neural network</a></h3><p>To improve the performance further, we can use a convolutional neural networks (CNN). See the <a href="http://cs231n.github.io/convolutional-networks/">course notes</a> by Andrej Karpathy for a good introduction to CNNs. We will implement the <a href="http://yann.lecun.com/exdb/lenet">LeNet</a> model which consists of two convolutional layers followed by two fully connected layers.</p><blockquote><p><a href="http://www.dataiku.com/blog/2015/08/18/Deep_Learning.html"><img src="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/images/le_net.png?raw=true" alt="image"/></a></p></blockquote><p>(<a href="http://www.dataiku.com/blog/2015/08/18/Deep_Learning.html">image source</a>)</p><p>Knet provides the <a href="reference.html#Knet.conv4"><code>conv4</code></a> and <a href="reference.html#Knet.pool"><code>pool</code></a> functions for the implementation of convolutional nets:</p><pre><code class="language-julia">function predict(w,x0)
    x1 = pool(relu.(conv4(w[1],x0) .+ w[2]))
    x2 = pool(relu.(conv4(w[3],x1) .+ w[4]))
    x3 = relu.(w[5]*mat(x2) .+ w[6])
    return w[7]*x3 .+ w[8]
end</code></pre><p>The weights for the convolutional net can be initialized as follows. </p><pre><code class="language-julia">w = Any[ xavier(Float32,5,5,1,20),  zeros(Float32,1,1,20,1),
         xavier(Float32,5,5,20,50), zeros(Float32,1,1,50,1),
         xavier(Float32,500,800),   zeros(Float32,500,1),
         xavier(Float32,10,500),    zeros(Float32,10,1) ]</code></pre><p>Here we used <a href="reference.html#Knet.xavier"><code>xavier</code></a> instead of <code>randn</code> which initializes weights based on their input and output widths.  </p><p>This model is larger and more expensive to train compared to the previous models we have seen. To perform the operations on the GPU, all we need to do is to convert our data and weights to <a href="reference.html#Knet.KnetArray"><code>KnetArray</code></a>s. <a href="reference.html#Knet.minibatch"><code>minibatch</code></a> takes an extra keyword argument <code>xtype</code> for this purpose, and we do it manually for the <code>w</code> weights:</p><pre><code class="language-julia">dtrn = minibatch(xtrn,ytrn,100,xtype=KnetArray)
dtst = minibatch(xtst,ytst,100,xtype=KnetArray)
w = map(KnetArray, w)</code></pre><p>The training proceeds as before giving us even better results. The code for the LeNet example can be found under <a href="https://github.com/denizyuret/Knet.jl/blob/master/examples/lenet">examples</a>.</p><pre><code class="language-julia">(:epoch, 0, :trn, 0.10435, :tst, 0.103)
(:epoch, 1, :trn, 0.98385, :tst, 0.9836)
...
(:epoch, 10, :trn, 0.9955166666666667, :tst, 0.9902)</code></pre><h3><a class="nav-anchor" id="Recurrent-neural-network-1" href="#Recurrent-neural-network-1">Recurrent neural network</a></h3><p>In this section we will see how to implement a recurrent neural network (RNN) in Knet. This example, like the last one, requires a GPU.  An RNN is a class of neural network where connections between units form a directed cycle, which allows them to keep a persistent state over time. This gives them the ability to process sequences of arbitrary length one element at a time, while keeping track of what happened at previous elements.</p><blockquote><p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs"><img src="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/images/RNN-unrolled.png?raw=true" alt="image"/></a></p></blockquote><p>(<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">image source</a>)</p><p>As an example, we will build a character-level language model inspired by <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness">&quot;The Unreasonable Effectiveness of Recurrent Neural Networks&quot;</a> from the Andrej Karpathy blog. The model can be trained with different genres of text, and can be used to generate original text in the same style.</p><p>We will use &quot;The Complete Works of William Shakespeare&quot; to train our model. The <code>shakespeare()</code> function defined in <code>gutenberg.jl</code> downloads the book and splits the data into 5M chars for training and 0.5M chars for testing.</p><pre><code class="language-julia">include(Knet.dir(&quot;data&quot;,&quot;gutenberg.jl&quot;))
trn,tst,chars = shakespeare()
map(summary,(trn,tst,chars))
# (&quot;4925284-element Array{UInt8,1}&quot;, &quot;525665-element Array{UInt8,1}&quot;, &quot;84-element Array{Char,1}&quot;)</code></pre><p>There are 84 unique characters in the data and they are mapped to UInt8 values in 1:84. The <code>chars</code> array can be used to recover the original text:</p><pre><code class="language-julia">println(string(chars[trn[1020:1210]]...))</code></pre><pre><code class="language-none">Cheated of feature by dissembling nature,
Deform&#39;d, unfinish&#39;d, sent before my time
Into this breathing world scarce half made up,
And that so lamely and unfashionable</code></pre><p>We minibatch the data into (256,100) blocks:</p><pre><code class="language-julia">BATCHSIZE = 256  # number of sequences per minibatch
SEQLENGTH = 100  # sequence length for bptt

function mb(a)
    N = div(length(a),BATCHSIZE)
    x = reshape(a[1:N*BATCHSIZE],N,BATCHSIZE)&#39; # reshape full data to (B,N) with contiguous rows
    minibatch(x[:,1:N-1], x[:,2:N], SEQLENGTH) # split into (B,T) blocks 
end

dtrn,dtst = mb(trn),mb(tst)
map(length, (dtrn,dtst))
# (192, 20)</code></pre><p>The <code>initmodel</code> function below initializes the weights for an RNN language model.  It returns a tuple where <code>r,w</code> are the RNN spec and weights, <code>wx</code> is the input embedding matrix, <code>wy,by</code> are the weight matrix and bias to produce the output from the hidden state. See <a href="reference.html#Knet.rnninit"><code>rnninit</code></a> for a full description of available options.</p><pre><code class="language-julia">RNNTYPE = :lstm  # can be :lstm, :gru, :tanh, :relu
NUMLAYERS = 1    # number of RNN layers
INPUTSIZE = 168  # size of the input character embedding
HIDDENSIZE = 334 # size of the hidden layers
VOCABSIZE = 84   # number of unique characters in data

function initmodel()
    w(d...)=KnetArray(xavier(Float32,d...))
    b(d...)=KnetArray(zeros(Float32,d...))
    r,wr = rnninit(INPUTSIZE,HIDDENSIZE,rnnType=RNNTYPE,numLayers=NUMLAYERS)
    wx = w(INPUTSIZE,VOCABSIZE)
    wy = w(VOCABSIZE,HIDDENSIZE)
    by = b(VOCABSIZE,1)
    return r,wr,wx,wy,by
end</code></pre><p>A character based language model needs to predict the next character in a piece of text given the current character and recent history as encoded in the internal state of the RNN. The <code>predict</code> function below takes weights <code>ws</code>, inputs <code>xs</code>, the initial hidden and cell states <code>hx</code> and <code>cx</code> and returns output scores <code>ys</code> along with the final hidden and cell states <code>hy</code> and <code>cy</code>. See <a href="reference.html#Knet.rnnforw"><code>rnnforw</code></a> for available options and the exact computations performed.</p><pre><code class="language-julia">function predict(ws,xs,hx,cx)
    r,wr,wx,wy,by = ws
    x = wx[:,xs]                                         # xs=(B,T) x=(X,B,T)
    y,hy,cy = rnnforw(r,wr,x,hx,cx,hy=true,cy=true)      # y=(H,B,T) hy=cy=(H,B,L)
    ys = by.+wy*reshape(y,size(y,1),size(y,2)*size(y,3)) # ys=(H,B*T)
    return ys, hy, cy
end</code></pre><p>The loss function returns the negative-log-likelihood from the predicted scores and updates the hidden and cell states <code>h</code> in-place. <a href="reference.html#AutoGrad.getval"><code>getval</code></a> is necessary to prevent AutoGrad state leaking from one minibatch to the next. We use <a href="reference.html#AutoGrad.gradloss"><code>gradloss</code></a> instead of <a href="reference.html#AutoGrad.grad"><code>grad</code></a> so that <code>lossgradient</code> returns both the gradient and the loss for reporting.</p><pre><code class="language-julia">function loss(w,x,y,h)
    py,hy,cy = predict(w,x,h...)
    h[1],h[2] = getval(hy),getval(cy)
    return nll(py,y)
end

lossgradient = gradloss(loss)</code></pre><p>Here is the <code>train</code> and <code>test</code> loops.  When hidden and cell values are set to nothing, <a href="reference.html#Knet.rnnforw"><code>rnnforw</code></a> assumes zero vectors.</p><pre><code class="language-julia">function train(model,data,optim)
    hiddens = Any[nothing,nothing]
    Σ,N=0,0
    for (x,y) in data
        grads,loss1 = lossgradient(model,x,y,hiddens)
        update!(model, grads, optim)
        Σ,N=Σ+loss1,N+1
    end
    return Σ/N
end

function test(model,data)
    hiddens = Any[nothing,nothing]
    Σ,N=0,0
    for (x,y) in data
        Σ,N = Σ+loss(model,x,y,hiddens),N+1
    end
    return Σ/N
end</code></pre><p>We are ready to initialize and train our model. We report train and test perplexity after every epoch. 30 epochs take less than 10 minutes with a K80 GPU:</p><pre><code class="language-julia">EPOCHS = 30
model = initmodel()
optim = optimizers(model, Adam)
@time for epoch in 1:EPOCHS
    @time trnloss = train(model,dtrn,optim) # ~18 seconds
    @time tstloss = test(model,dtst)        # ~0.5 seconds
    println((:epoch, epoch, :trnppl, exp(trnloss), :tstppl, exp(tstloss)))
end</code></pre><p>To generate text we sample each character randomly using the probabilities predicted by the model based on the previous character. The helper function <code>sample</code> takes unnormalized scores <code>y</code> and samples an index based on normalized probabilities based on <code>y</code>. The first character is initialized to newline and <code>n</code> characters are sampled based on the model.</p><pre><code class="language-julia">function generate(model,n)
    function sample(y)
        p,r=Array(exp.(y-logsumexp(y))),rand()
        for j=1:length(p); (r -= p[j]) &lt; 0 &amp;&amp; return j; end
    end
    h,c = nothing,nothing
    x = findfirst(chars,&#39;\n&#39;)
    for i=1:n
        y,h,c = predict(model,[x],h,c)
        x = sample(y)
        print(chars[x])
    end
    println()
end

generate(model,1000)</code></pre><p>Here is a random sample of 1000 characters from the model.  Note that the model has learnt to generate person names, correct indentation and mostly English words only by reading Shakespeare one letter at a time! The code for this example is available in the <a href="https://github.com/denizyuret/Knet.jl/tree/master/examples/charlm">charlm</a> notebook.</p><pre><code class="language-none">    Pand soping them, my lord, if such a foolish?
  MARTER. My lord, and nothing in England&#39;s ground to new comp&#39;d.
    To bless your view of wot their dullst. If Doth no ape;
    Which with the heart. Rome father stuff
    These shall sweet Mary against a sudden him
    Upon up th&#39; night is a wits not that honour,
    Shouts have sure?
  MACBETH. Hark? And, Halcance doth never memory I be thou what
    My enties mights in Tim thou?
  PIESTO. Which it time&#39;s purpose mine hortful and
    is my Lord.
  BOTTOM. My lord, good mine eyest, then: I will not set up.
  LUCILIUS. Who shall</code></pre><h2><a class="nav-anchor" id="Benchmarks-1" href="#Benchmarks-1">Benchmarks</a></h2><p>Each of the examples above was used as a benchmark to compare Knet with other frameworks. The table below shows the number of seconds it takes to train a given model for a particular dataset, number of epochs and minibatch size for Knet, Theano, Torch, Caffe and TensorFlow. Knet had comparable performance to other commonly used frameworks.</p><p>Knet Benchmarks (Sep 30, 2016):</p><table><tr><th>model</th><th>dataset</th><th>epochs</th><th>batch</th><th>Knet</th><th>Theano</th><th>Torch</th><th>Caffe</th><th>TFlow</th></tr><tr><td>LinReg</td><td>Housing</td><td>10K</td><td>506</td><td>2.84</td><td>1.88</td><td>2.66</td><td>2.35</td><td>5.92</td></tr><tr><td>Softmax</td><td>MNIST</td><td>10</td><td>100</td><td>2.35</td><td>1.40</td><td>2.88</td><td>2.45</td><td>5.57</td></tr><tr><td>MLP</td><td>MNIST</td><td>10</td><td>100</td><td>3.68</td><td>2.31</td><td>4.03</td><td>3.69</td><td>6.94</td></tr><tr><td>LeNet</td><td>MNIST</td><td>1</td><td>100</td><td>3.59</td><td>3.03</td><td>1.69</td><td>3.54</td><td>8.77</td></tr><tr><td>CharLM</td><td>Hiawatha</td><td>1</td><td>128</td><td>2.25</td><td>2.42</td><td>2.23</td><td>1.43</td><td>2.86</td></tr></table><p>The benchmarking was done on g2.2xlarge GPU instances on Amazon AWS. The code is available at <a href="https://github.com/ozanarkancan/Knet8-Benchmarks">github</a> and as machine image <code>deep_AMI_v6</code> at AWS N.California. See the section on <a href="install.html#Using-Amazon-AWS-1">Using Amazon AWS</a> for more information. The datasets are available online using the following links: <a href="https://archive.ics.uci.edu/ml/datasets/Housing">Housing</a>, <a href="http://yann.lecun.com/exdb/mnist">MNIST</a>, <a href="http://www.gutenberg.org/files/19/19.txt">Hiawatha</a>. The MLP uses a single hidden layer of 64 units. CharLM uses a single layer LSTM language model with embedding and hidden layer sizes set to 256 and trained using BPTT with a sequence length of 100. Each dataset was minibatched and transferred to GPU prior to benchmarking when possible.</p><p>More recently (Nov 24, 2017), @ilkarman has published CNN and RNN <a href="https://github.com/ilkarman/DeepLearningFrameworks">benchmarks</a> on Nvidia K80 GPUs, using the Microsoft Azure Data Science Virtual Machine for Linux (Ubuntu):</p><p>Training CNN (VGG-style) on CIFAR-10 - Image Recognition (Nov 24, 2017)</p><table><tr><th>DL Library</th><th>Test Accuracy (%)</th><th>Training Time (s)</th></tr><tr><td><a href="MXNet_CNN.ipynb">MXNet</a></td><td>77</td><td>145</td></tr><tr><td><a href="Caffe2_CNN.ipynb">Caffe2</a></td><td>79</td><td>148</td></tr><tr><td><a href="Gluon_CNN.ipynb">Gluon</a></td><td>76</td><td>152</td></tr><tr><td><a href="Knet_CNN.ipynb">Knet(Julia)</a></td><td>78</td><td>153</td></tr><tr><td><a href="Chainer_CNN.ipynb">Chainer</a></td><td>79</td><td>162</td></tr><tr><td><a href="CNTK_CNN.ipynb">CNTK</a></td><td>78</td><td>163</td></tr><tr><td><a href="PyTorch_CNN.ipynb">PyTorch</a></td><td>78</td><td>169</td></tr><tr><td><a href="Tensorflow_CNN.ipynb">Tensorflow</a></td><td>78</td><td>173</td></tr><tr><td><a href="Keras_CNTK_CNN.ipynb">Keras(CNTK)</a></td><td>77</td><td>194</td></tr><tr><td><a href="Keras_TF_CNN.ipynb">Keras(TF)</a></td><td>77</td><td>241</td></tr><tr><td><a href="Theano_Lasagne_CNN.ipynb">Lasagne(Theano)</a></td><td>77</td><td>253</td></tr><tr><td><a href="Keras_Theano_CNN.ipynb">Keras(Theano)</a></td><td>78</td><td>269</td></tr></table><p>Training RNN (GRU) on IMDB - Natural Language Processing (Sentiment Analysis) (Nov 24, 2017)</p><table><tr><th>DL Library</th><th>Test Accuracy (%)</th><th>Training Time (s)</th><th>Using CuDNN?</th></tr><tr><td><a href="MXNet_RNN.ipynb">MXNet</a></td><td>86</td><td>29</td><td>Yes</td></tr><tr><td><a href="Tensorflow_RNN.ipynb">Tensorflow</a></td><td>86</td><td>30</td><td>Yes</td></tr><tr><td><a href="Knet_RNN.ipynb">Knet(Julia)</a></td><td>85</td><td>30</td><td>Yes</td></tr><tr><td><a href="PyTorch_RNN.ipynb">Pytorch</a></td><td>86</td><td>31</td><td>Yes</td></tr><tr><td><a href="CNTK_RNN.ipynb">CNTK</a></td><td>85</td><td>32</td><td>Yes</td></tr><tr><td><a href="Keras_TF_RNN.ipynb">Keras(TF)</a></td><td>86</td><td>35</td><td>Yes</td></tr><tr><td><a href="Keras_CNTK_RNN.ipynb">Keras(CNTK)</a></td><td>86</td><td>86</td><td>No Available</td></tr></table><h2><a class="nav-anchor" id="Under-the-hood-1" href="#Under-the-hood-1">Under the hood</a></h2><p>Knet relies on the <a href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad</a> package and the <a href="reference.html#KnetArray-1">KnetArray</a> data type for its functionality and performance. AutoGrad computes the gradient of Julia functions and KnetArray implements high performance GPU arrays with custom memory management. This section briefly describes them.</p><h3><a class="nav-anchor" id="KnetArrays-1" href="#KnetArrays-1">KnetArrays</a></h3><p>GPUs have become indispensable for training large deep learning models. Even the small examples implemented here run up to 17x faster on the GPU compared to the 8 core CPU architecture we use for benchmarking. However GPU implementations have a few potential pitfalls: (i) GPU memory allocation is slow, (ii) GPU-RAM memory transfer is slow, (iii) reduction operations (like <code>sum</code>) can be very slow unless implemented properly (See <a href="http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf">Optimizing Parallel Reduction in CUDA</a>).</p><p>Knet implements <a href="reference.html#KnetArray-1">KnetArray</a> as a Julia data type that wraps GPU array pointers. KnetArray is based on the more standard <a href="https://github.com/JuliaGPU/CUDArt.jl">CudaArray</a> with a few important differences: (i) KnetArrays have a custom memory manager, similar to <a href="http://arrayfire.com">ArrayFire</a>, which reuse pointers garbage collected by Julia to reduce the number of GPU memory allocations, (ii) array ranges (e.g. <code>a[:,3:5]</code>) are handled as views with shared pointers instead of copies when possible, and (iii) a number of custom CUDA kernels written for KnetArrays implement element-wise, broadcasting, and scalar and vector reduction operations efficiently. As a result Knet allows users to implement their models using high-level code, yet be competitive in performance with other frameworks as demonstrated in the benchmarks section.</p><h3><a class="nav-anchor" id="AutoGrad-1" href="#AutoGrad-1">AutoGrad</a></h3><p>As we have seen, many common machine learning models can be expressed as differentiable programs that input parameters and data and output a scalar loss value. The loss value measures how close the model predictions are to desired values with the given parameters. Training a model can then be seen as an optimization problem: find the parameters that minimize the loss. Typically, a gradient based optimization algorithm is used for computational efficiency: the direction in the parameter space in which the loss reduction is maximum is given by the negative gradient of the loss with respect to the parameters. Thus gradient computations take a central stage in software frameworks for machine learning. In this section I will briefly outline existing gradient computation techniques and motivate the particular approach taken by Knet.</p><p>Computation of gradients in computer models is performed by four main methods <a href="https://arxiv.org/abs/1502.05767">(Baydin et al. 2015)</a>:</p><ul><li><p>manual differentiation (programming the derivatives)</p></li><li><p>numerical differentiation (using finite difference approximations)</p></li><li><p>symbolic differentiation (using expression manipulation)</p></li><li><p>automatic differentiation (detailed below)</p></li></ul><p>Manually taking derivatives and coding the result is labor intensive, error-prone, and all but impossible with complex deep learning models. Numerical differentiation is simple: <span>$f&#39;(x)=(f(x+\epsilon)-f(x-\epsilon))/(2\epsilon)$</span> but impractical: the finite difference equation needs to be evaluated for each individual parameter, of which there are typically many. Pure symbolic differentiation using expression manipulation, as implemented in software such as Maxima, Maple, and Mathematica is impractical for different reasons: (i) it may not be feasible to express a machine learning model as a closed form mathematical expression, and (ii) the symbolic derivative can be exponentially larger than the model itself leading to inefficient run-time calculation. This leaves us with automatic differentiation.</p><p>Automatic differentiation is the idea of using symbolic derivatives only at the level of elementary operations, and computing the gradient of a compound function by applying the chain rule to intermediate numerical results. For example, pure symbolic differentiation of <span>$\sin^2(x)$</span> could give us <span>$2\sin(x)\cos(x)$</span> directly. Automatic differentiation would use the intermediate numerical values <span>$x_1=\sin(x)$</span>, <span>$x_2=x_1^2$</span> and the elementary derivatives <span>$dx_2/dx_1=2x_1$</span>, <span>$dx_1/dx=\cos(x)$</span> to compute the same answer without ever building a full gradient expression.</p><p>To implement automatic differentiation the target function needs to be decomposed into its elementary operations, a process similar to compilation. Most machine learning frameworks (such as Theano, Torch, Caffe, Tensorflow and older versions of Knet prior to v0.8) compile models expressed in a restricted mini-language into a static computational graph of elementary operations that have pre-defined derivatives. There are two drawbacks with this approach: (i) the restricted mini-languages tend to have limited support for high-level language features such as conditionals, loops, helper functions, array indexing, etc. (e.g. the infamous <code>scan</code> operation in Theano) (ii) the sequence of elementary operations that unfold at run-time needs to be known in advance, and they are difficult to handle when the sequence is data dependent.</p><p>There is an alternative: high-level languages, like Julia and Python, already know how to decompose functions into their elementary operations. If we let the users define their models directly in a high-level language, then record the elementary operations during loss calculation at run-time, a dynamic computational graph can be constructed from the recorded operations. The cost of recording is not prohibitive: The table below gives cumulative times for elementary operations of an MLP with quadratic loss. Recording only adds 15% to the raw cost of the forward computation. Backpropagation roughly doubles the total time as expected.</p><table><tr><th>op</th><th>secs</th></tr><tr><td><code>a1=w1*x</code></td><td>0.67</td></tr><tr><td><code>a2=w2.+a1</code></td><td>0.71</td></tr><tr><td><code>a3=max(0,a2)</code></td><td>0.75</td></tr><tr><td><code>a4=w3*a3</code></td><td>0.81</td></tr><tr><td><code>a5=w4.+a4</code></td><td>0.85</td></tr><tr><td><code>a6=a5-y</code></td><td>0.89</td></tr><tr><td><code>a7=sumabs2(a6)</code></td><td>1.18</td></tr><tr><td>+recording</td><td>1.33</td></tr><tr><td>+backprop</td><td>2.79</td></tr></table><p>This is the approach taken by the popular <a href="https://github.com/HIPS/autograd">autograd</a> Python package and its Julia port <a href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad.jl</a> used by Knet. Recently, other machine learning frameworks have been adapting dynamic computational graphs: <a href="http://docs.chainer.org/en/stable/index.html">Chainer</a>, <a href="https://arxiv.org/abs/1701.03980">DyNet</a>, <a href="https://github.com/pytorch/pytorch">PyTorch</a>, <a href="https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html">TensorFlow Fold</a>.</p><p>In Knet <code>g=grad(f)</code> generates a gradient function <code>g</code>, which takes the same inputs as the function <code>f</code> but returns the gradient. The gradient function <code>g</code> triggers recording by boxing the parameters in a special data type and calls <code>f</code>. The elementary operations in <code>f</code> are overloaded to record their actions and output boxed answers when their inputs are boxed. The sequence of recorded operations is then used to compute gradients. In the Julia AutoGrad package, derivatives can be defined independently for each method of a function (determined by argument types) making full use of Julia&#39;s multiple dispatch. New elementary operations and derivatives can be defined concisely using Julia&#39;s macro and meta-programming facilities. See <a href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad.jl</a> for details.</p><h2><a class="nav-anchor" id="Contributing-1" href="#Contributing-1">Contributing</a></h2><p>Knet is an open-source project and we are always open to new contributions: bug reports and fixes, feature requests and contributions, new machine learning models and operators, inspiring examples, benchmarking results are all welcome. If you would like to contribute to Knet development, check out the <a href="https://groups.google.com/forum/#!forum/knet-dev">knet-dev</a> mailing list and <a href="install.html#Tips-for-developers-1">Tips for developers</a>.</p><p>Current contributors:</p><ul><li><p>Deniz Yuret</p></li><li><p>Ozan Arkan Can</p></li><li><p>Onur Kuru</p></li><li><p>Emre Ünal</p></li><li><p>Erenay Dayanık</p></li><li><p>Ömer Kırnap</p></li><li><p>İlker Kesen</p></li><li><p>Emre Yolcu</p></li><li><p>Meriç Melike Softa</p></li><li><p>Ekrem Emre Yurdakul</p></li><li><p>Enis Berk</p></li><li><p>Can Gümeli</p></li><li><p>Carlo Lucibello</p></li><li><p>张实唯 (@ylxdzsw)</p></li></ul><footer><hr/><a class="previous" href="install.html"><span class="direction">Previous</span><span class="title">Setting up Knet</span></a><a class="next" href="reference.html"><span class="direction">Next</span><span class="title">Reference</span></a></footer></article></body></html>
