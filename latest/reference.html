<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · Knet.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Knet.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="install.html">Setting up Knet</a></li><li><a class="toctext" href="tutorial.html">Introduction to Knet</a></li><li class="current"><a class="toctext" href="reference.html">Reference</a><ul class="internal"><li><a class="toctext" href="#AutoGrad-1">AutoGrad</a></li><li><a class="toctext" href="#KnetArray-1">KnetArray</a></li><li><a class="toctext" href="#Utilities-1">Utilities</a></li><li><a class="toctext" href="#Convolution-and-Pooling-1">Convolution and Pooling</a></li><li><a class="toctext" href="#Recurrent-neural-networks-1">Recurrent neural networks</a></li><li><a class="toctext" href="#Optimization-methods-1">Optimization methods</a></li><li><a class="toctext" href="#Hyperparameter-optimization-1">Hyperparameter optimization</a></li><li><a class="toctext" href="#Initialization-1">Initialization</a></li><li><a class="toctext" href="#AutoGrad-(advanced)-1">AutoGrad (advanced)</a></li><li><a class="toctext" href="#Function-Index-1">Function Index</a></li></ul></li></ul></li><li><span class="toctext">Textbook</span><ul><li><a class="toctext" href="backprop.html">Backpropagation</a></li><li><a class="toctext" href="softmax.html">Softmax Classification</a></li><li><a class="toctext" href="mlp.html">Multilayer Perceptrons</a></li><li><a class="toctext" href="cnn.html">Convolutional Neural Networks</a></li><li><a class="toctext" href="rnn.html">Recurrent Neural Networks</a></li><li><a class="toctext" href="rl.html">Reinforcement Learning</a></li><li><a class="toctext" href="opt.html">Optimization</a></li><li><a class="toctext" href="gen.html">Generalization</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="reference.html">Reference</a></li></ul><a class="edit-page" href="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/reference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Reference</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h1><p><strong>Contents</strong></p><ul><li><a href="reference.html#Reference-1">Reference</a></li><ul><li><a href="reference.html#AutoGrad-1">AutoGrad</a></li><li><a href="reference.html#KnetArray-1">KnetArray</a></li><li><a href="reference.html#Utilities-1">Utilities</a></li><li><a href="reference.html#Convolution-and-Pooling-1">Convolution and Pooling</a></li><li><a href="reference.html#Recurrent-neural-networks-1">Recurrent neural networks</a></li><li><a href="reference.html#Optimization-methods-1">Optimization methods</a></li><li><a href="reference.html#Hyperparameter-optimization-1">Hyperparameter optimization</a></li><li><a href="reference.html#Initialization-1">Initialization</a></li><li><a href="reference.html#AutoGrad-(advanced)-1">AutoGrad (advanced)</a></li><li><a href="reference.html#Function-Index-1">Function Index</a></li></ul></ul><h2><a class="nav-anchor" id="AutoGrad-1" href="#AutoGrad-1">AutoGrad</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.grad" href="#AutoGrad.grad"><code>AutoGrad.grad</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">grad(fun, argnum=1)</code></pre><p>Take a function <code>fun(X...)-&gt;Y</code> and return another function <code>gfun(X...)-&gt;dXi</code> which computes its gradient with respect to positional argument number <code>argnum</code>. The function <code>fun</code> should be scalar-valued. The returned function <code>gfun</code> takes the same arguments as <code>fun</code>, but returns the gradient instead. The gradient has the same type and size as the target argument which can be a Number, Array, Tuple, or Dict.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/blob/0c27dd85875a8c0cdbc9a4dd11ffbf72493ecbc2/src/core.jl#L23-L35">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gradloss" href="#AutoGrad.gradloss"><code>AutoGrad.gradloss</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">gradloss(fun, argnum=1)</code></pre><p>Another version of <code>grad</code> where the generated function returns a (gradient,value) pair.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/blob/0c27dd85875a8c0cdbc9a4dd11ffbf72493ecbc2/src/core.jl#L46-L53">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.gradcheck" href="#AutoGrad.gradcheck"><code>AutoGrad.gradcheck</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">gradcheck(f, w, x...; kwargs...)</code></pre><p>Numerically check the gradient of <code>f(w,x...;o...)</code> with respect to its first argument <code>w</code> and return a boolean result.</p><p>The argument <code>w</code> can be a Number, Array, Tuple or Dict which in turn can contain other Arrays etc.  Only the largest 10 entries in each numerical gradient array are checked by default.  If the output of f is not a number, gradcheck constructs and checks a scalar function by taking its dot product with a random vector.</p><p><strong>Keywords</strong></p><ul><li><p><code>gcheck=10</code>: number of largest entries from each numeric array in gradient <code>dw=(grad(f))(w,x...;o...)</code> compared to their numerical estimates.</p></li><li><p><code>verbose=false</code>: print detailed messages if true.</p></li><li><p><code>kwargs=[]</code>: keyword arguments to be passed to <code>f</code>.</p></li><li><p><code>delta=atol=rtol=cbrt(eps(w))</code>: tolerance parameters.  See <code>isapprox</code> for their meaning.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/blob/0c27dd85875a8c0cdbc9a4dd11ffbf72493ecbc2/src/gradcheck.jl#L8-L34">source</a></section><h2><a class="nav-anchor" id="KnetArray-1" href="#KnetArray-1">KnetArray</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.KnetArray" href="#Knet.KnetArray"><code>Knet.KnetArray</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">KnetArray{T}(dims)
KnetArray(a::AbstractArray)
Array(k::KnetArray)</code></pre><p>Container for GPU arrays that supports most of the AbstractArray interface.  The constructor allocates a KnetArray in the currently active device, as specified by <code>gpu()</code>.  KnetArrays and Arrays can be converted to each other as shown above, which involves copying to and from the GPU memory.  Only Float32/64 KnetArrays are fully supported.</p><p>Important differences from the alternative CudaArray are: (1) a custom memory manager that minimizes the number of calls to the slow cudaMalloc by reusing already allocated but garbage collected GPU pointers.  (2) a custom getindex that handles ranges such as <code>a[5:10]</code> as views with shared memory instead of copies.  (3) custom CUDA kernels that implement elementwise, broadcasting, and reduction operations.</p><p><strong>Supported functions:</strong></p><ul><li><p>Indexing: getindex, setindex! with the following index types:</p><ul><li><p>1-D: Real, Colon, OrdinalRange, AbstractArray{Real}, AbstractArray{Bool}, CartesianIndex, AbstractArray{CartesianIndex}, EmptyArray, KnetArray{Int32} (low level), KnetArray{0/1} (using float for BitArray) (1-D includes linear indexing of multidimensional arrays)</p></li><li><p>2-D: (Colon,Union{Real,Colon,OrdinalRange,AbstractVector{Real},AbstractVector{Bool},KnetVector{Int32}}), (Union{Real,AbstractUnitRange,Colon}...) (in any order)</p></li><li><p>N-D: (Real...)</p></li></ul></li><li><p>Array operations: ==, !=, cat, convert, copy, copy!, deepcopy, display, eachindex, eltype, endof, fill!, first, hcat, isapprox, isempty, length, ndims, ones, pointer, rand!, randn!, reshape, similar, size, stride, strides, summary, vcat, vec, zeros. (cat(i,x,y) supported for i=1,2.)</p></li><li><p>Math operators: (-), abs, abs2, acos, acosh, asin, asinh, atan, atanh, cbrt, ceil, cos, cosh, cospi, erf, erfc, erfcinv, erfcx, erfinv, exp, exp10, exp2, expm1, floor, log, log10, log1p, log2, round, sign, sin, sinh, sinpi, sqrt, tan, tanh, trunc</p></li><li><p>Broadcasting operators: (.*), (.+), (.-), (./), (.&lt;), (.&lt;=), (.!=), (.==), (.&gt;), (.&gt;=), (.^), max, min.  (Boolean operators generate outputs with same type as inputs; no support for KnetArray{Bool}.)</p></li><li><p>Reduction operators: countnz, maximum, mean, minimum, prod, sum, sumabs, sumabs2, vecnorm.</p></li><li><p>Linear algebra: (*), axpy!, permutedims (up to 5D), transpose</p></li><li><p>Knet extras: relu, sigm, invx, logp, logsumexp, conv4, pool, deconv4, unpool, mat, update! (Only 4D/5D, Float32/64 KnetArrays support conv4, pool, deconv4, unpool)</p></li></ul><p><strong>Memory management</strong></p><p>Knet models do not overwrite arrays which need to be preserved for gradient calculation.  This leads to a lot of allocation and regular GPU memory allocation is prohibitively slow. Fortunately most models use identically sized arrays over and over again, so we can minimize the number of actual allocations by reusing preallocated but garbage collected pointers.</p><p>When Julia gc reclaims a KnetArray, a special finalizer keeps its pointer in a table instead of releasing the memory.  If an array with the same size in bytes is later requested, the same pointer is reused. The exact algorithm for allocation is:</p><ol><li><p>Try to find a previously allocated and garbage collected pointer in the current device. (0.5 μs)</p></li><li><p>If not available, try to allocate a new array using cudaMalloc. (10 μs)</p></li><li><p>If not successful, try running gc() and see if we get a pointer of the right size. (75 ms, but this should be amortized over all reusable pointers that become available due to the gc)</p></li><li><p>Finally if all else fails, clean up all saved pointers in the current device using cudaFree and try allocation one last time. (25-70 ms, however this causes the elimination of all reusable pointers)</p></li></ol></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/karray.jl#L1-L81">source</a></section><h2><a class="nav-anchor" id="Utilities-1" href="#Utilities-1">Utilities</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.accuracy" href="#Knet.accuracy"><code>Knet.accuracy</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">accuracy(scores, answers, d=1; average=true)</code></pre><p>Given an unnormalized <code>scores</code> matrix and an <code>Integer</code> array of correct <code>answers</code>, return the ratio of instances where the correct answer has the maximum score. <code>d=1</code> means instances are in columns, <code>d=2</code> means instances are in rows. Use <code>average=false</code> to return the number of correct answers instead of the ratio.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/loss.jl#L107-L117">source</a><div><pre><code class="language-none">accuracy(model, data, predict; average=true)</code></pre><p>Compute <code>accuracy(predict(model,x), y)</code> for <code>(x,y)</code> in <code>data</code> and return the ratio (if average=true) or the count (if average=false) of correct answers.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/data.jl#L69-L76">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.dir" href="#Knet.dir"><code>Knet.dir</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">Knet.dir(path...)</code></pre><p>Construct a path relative to Knet root.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; Knet.dir(&quot;examples&quot;,&quot;mnist.jl&quot;)
&quot;/home/dyuret/.julia/v0.5/Knet/examples/mnist.jl&quot;</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/Knet.jl#L39-L49">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.dropout" href="#Knet.dropout"><code>Knet.dropout</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">dropout(x, p)</code></pre><p>Given an array <code>x</code> and probability <code>0&lt;=p&lt;=1</code>, just return <code>x</code> if testing, return an array <code>y</code> in which each element is 0 with probability <code>p</code> or <code>x[i]/(1-p)</code> with probability <code>1-p</code> if training. Training mode is detected automatically based on the type of <code>x</code>, which is <code>AutoGrad.Rec</code> during gradient calculation.  Use the keyword argument <code>training::Bool</code> to change the default mode and <code>seed::Number</code> to set the random number seed for reproducible results. See <a href="http://jmlr.org/papers/v15/srivastava14a.html">(Srivastava et al. 2014)</a>  for a reference.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/dropout.jl#L1-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.gpu" href="#Knet.gpu"><code>Knet.gpu</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>gpu()</code> returns the id of the active GPU device or -1 if none are active.</p><p><code>gpu(true)</code> resets all GPU devices and activates the one with the most available memory.</p><p><code>gpu(false)</code> resets and deactivates all GPU devices.</p><p><code>gpu(d::Int)</code> activates the GPU device <code>d</code> if <code>0 &lt;= d &lt; gpuCount()</code>, otherwise deactivates devices.</p><p><code>gpu(true/false)</code> resets all devices.  If there are any allocated KnetArrays their pointers will be left dangling.  Thus <code>gpu(true/false)</code> should only be used during startup.  If you want to suspend GPU use temporarily, use <code>gpu(-1)</code>.</p><p><code>gpu(d::Int)</code> does not reset the devices.  You can select a previous device and find allocated memory preserved.  However trying to operate on arrays of an inactive device will result in error.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/gpu.jl#L49-L71">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.invx" href="#Knet.invx"><code>Knet.invx</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>invx(x) = (1./x)</code></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/unary.jl#L138">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.knetgc" href="#Knet.knetgc"><code>Knet.knetgc</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">knetgc(dev=gpu())</code></pre><p>cudaFree all pointers allocated on device <code>dev</code> that were previously allocated and garbage collected. Normally Knet holds on to all garbage collected pointers for reuse. Try this if you run out of GPU memory.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/kptr.jl#L111-L119">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.logp" href="#Knet.logp"><code>Knet.logp</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">logp(x,[dims])</code></pre><p>Treat entries in <code>x</code> as as unnormalized log probabilities and return normalized log probabilities.</p><p><code>dims</code> is an optional argument, if not specified the normalization is over the whole <code>x</code>, otherwise the normalization is performed over the given dimensions.  In particular, if <code>x</code> is a matrix, <code>dims=1</code> normalizes columns of <code>x</code> and <code>dims=2</code> normalizes rows of <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/loss.jl#L1-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.logsumexp" href="#Knet.logsumexp"><code>Knet.logsumexp</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">logsumexp(x,[dims])</code></pre><p>Compute <code>log(sum(exp(x),dims))</code> in a numerically stable manner.</p><p><code>dims</code> is an optional argument, if not specified the summation is over the whole <code>x</code>, otherwise the summation is performed over the given dimensions.  In particular if <code>x</code> is a matrix, <code>dims=1</code> sums columns of <code>x</code> and <code>dims=2</code> sums rows of <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/loss.jl#L69-L80">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.minibatch" href="#Knet.minibatch"><code>Knet.minibatch</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">minibatch(x, y, batchsize; shuffle, partial, xtype, ytype)</code></pre><p>Return an iterable of minibatches [(xi,yi)...] given data tensors x, y and batchsize.  The last dimension of x and y should match and give the number of instances. Keyword arguments:</p><ul><li><p><code>shuffle=false</code>: Shuffle the instances before minibatching.</p></li><li><p><code>partial=false</code>: If true include the last partial minibatch &lt; batchsize.</p></li><li><p><code>xtype=typeof(x)</code>: Convert xi in minibatches to this type.</p></li><li><p><code>ytype=typeof(y)</code>: Convert yi in minibatches to this type.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/data.jl#L4-L17">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.nll" href="#Knet.nll"><code>Knet.nll</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">nll(scores, answers, d=1; average=true)</code></pre><p>Given an unnormalized <code>scores</code> matrix and an <code>Integer</code> array of correct <code>answers</code>, return the per-instance negative log likelihood. <code>d=1</code> means instances are in columns, <code>d=2</code> means instances are in rows.  Use <code>average=false</code> to return the sum instead of per-instance average.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/loss.jl#L89-L99">source</a><div><pre><code class="language-none">nll(model, data, predict; average=true)</code></pre><p>Compute <code>nll(predict(model,x), y)</code> for <code>(x,y)</code> in <code>data</code> and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/data.jl#L51-L58">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.relu" href="#Knet.relu"><code>Knet.relu</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>relu(x) = max(0,x)</code></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/unary.jl#L139">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.setseed" href="#Knet.setseed"><code>Knet.setseed</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">setseed(n::Integer)</code></pre><p>Run srand(n) on both cpu and gpu.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/random.jl#L32-L36">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.sigm" href="#Knet.sigm"><code>Knet.sigm</code></a> — <span class="docstring-category">Function</span>.</div><div><p><code>sigm(x) = (1./(1+exp(-x)))</code></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/unary.jl#L140">source</a></section><h2><a class="nav-anchor" id="Convolution-and-Pooling-1" href="#Convolution-and-Pooling-1">Convolution and Pooling</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.conv4" href="#Knet.conv4"><code>Knet.conv4</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">conv4(w, x; kwargs...)</code></pre><p>Execute convolutions or cross-correlations using filters specified with <code>w</code> over tensor <code>x</code>.</p><p>Currently KnetArray{Float32/64,4/5} and Array{Float32/64,4} are supported as <code>w</code> and <code>x</code>.  If <code>w</code> has dimensions <code>(W1,W2,...,I,O)</code> and <code>x</code> has dimensions <code>(X1,X2,...,I,N)</code>, the result <code>y</code> will have dimensions <code>(Y1,Y2,...,O,N)</code> where</p><pre><code class="language-none">Yi=1+floor((Xi+2*padding[i]-Wi)/stride[i])</code></pre><p>Here <code>I</code> is the number of input channels, <code>O</code> is the number of output channels, <code>N</code> is the number of instances, and <code>Wi,Xi,Yi</code> are spatial dimensions.  <code>padding</code> and <code>stride</code> are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.</p><p><strong>Keywords</strong></p><ul><li><p><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</p></li><li><p><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</p></li><li><p><code>upscale=1</code>: upscale factor for each dimension.</p></li><li><p><code>mode=0</code>: 0 for convolution and 1 for cross-correlation.</p></li><li><p><code>alpha=1</code>: can be used to scale the result.</p></li><li><p><code>handle</code>: handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/conv.jl#L1-L31">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.deconv4" href="#Knet.deconv4"><code>Knet.deconv4</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Deconvolution; <code>reverse</code> of convolution.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/conv.jl#L168-L172">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.mat" href="#Knet.mat"><code>Knet.mat</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">mat(x)</code></pre><p>Reshape x into a two-dimensional matrix.</p><p>This is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.  For 1-D inputs returns <code>reshape(x, (length(x),1))</code>.  For inputs with more than two dimensions of size <code>(X1,X2,...,XD)</code>, returns</p><pre><code class="language-none">reshape(x, (X1*X2*...*X[D-1],XD))</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/linalg.jl#L108-L121">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.pool" href="#Knet.pool"><code>Knet.pool</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">pool(x; kwargs...)</code></pre><p>Compute pooling of input values (i.e., the maximum or average of several adjacent values) to produce an output with smaller height and/or width.  </p><p>Currently 4 or 5 dimensional KnetArrays with <code>Float32</code> or <code>Float64</code> entries are supported.  If <code>x</code> has dimensions <code>(X1,X2,...,I,N)</code>, the result <code>y</code> will have dimensions <code>(Y1,Y2,...,I,N)</code> where</p><pre><code class="language-none">Yi=1+floor((Xi+2*padding[i]-window[i])/stride[i])</code></pre><p>Here <code>I</code> is the number of input channels, <code>N</code> is the number of instances, and <code>Xi,Yi</code> are spatial dimensions.  <code>window</code>, <code>padding</code> and <code>stride</code> are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.</p><p><strong>Keywords:</strong></p><ul><li><p><code>window=2</code>: the pooling window size for each dimension.</p></li><li><p><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</p></li><li><p><code>stride=window</code>: the number of elements to slide to reach the next pooling window.</p></li><li><p><code>mode=0</code>: 0 for max, 1 for average including padded values, 2 for average excluding padded values.</p></li><li><p><code>maxpoolingNanOpt=0</code>: Nan numbers are not propagated if 0, they are propagated if 1.</p></li><li><p><code>alpha=1</code>: can be used to scale the result.</p></li><li><p><code>handle</code>: Handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/conv.jl#L91-L121">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.unpool" href="#Knet.unpool"><code>Knet.unpool</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Unpooling; <code>reverse</code> of pooling.</p><pre><code class="language-none">x == pool(unpool(x;o...); o...)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/conv.jl#L146-L152">source</a></section><h2><a class="nav-anchor" id="Recurrent-neural-networks-1" href="#Recurrent-neural-networks-1">Recurrent neural networks</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.rnninit" href="#Knet.rnninit"><code>Knet.rnninit</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">rnninit(inputSize, hiddenSize; opts...)</code></pre><p>Return an <code>(r,w)</code> pair where <code>r</code> is a RNN struct and <code>w</code> is a single weight array that includes all matrices and biases for the RNN. Keyword arguments:</p><ul><li><p><code>rnnType=:lstm</code> Type of RNN: One of :relu, :tanh, :lstm, :gru.</p></li><li><p><code>numLayers=1</code>: Number of RNN layers.</p></li><li><p><code>bidirectional=false</code>: Create a bidirectional RNN if <code>true</code>.</p></li><li><p><code>dropout=0.0</code>: Dropout probability. Ignored if <code>numLayers==1</code>.</p></li><li><p><code>skipInput=false</code>: Do not multiply the input with a matrix if <code>true</code>.</p></li><li><p><code>dataType=Float32</code>: Data type to use for weights.</p></li><li><p><code>algo=0</code>: Algorithm to use, see CUDNN docs for details.</p></li><li><p><code>seed=0</code>: Random number seed. Uses <code>time()</code> if 0.</p></li><li><p><code>winit=xavier</code>: Weight initialization method for matrices.</p></li><li><p><code>binit=zeros</code>: Weight initialization method for bias vectors.</p></li></ul><p>RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:</p><p><code>:relu</code> and <code>:tanh</code>: Single gate RNN with activation function f:</p><pre><code class="language-none">h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)</code></pre><p><code>:gru</code>: Gated recurrent unit:</p><pre><code class="language-none">i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate
r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate
n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate
h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]</code></pre><p><code>:lstm</code>: Long short term memory unit with no peephole connections:</p><pre><code class="language-none">i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate
f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate
o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate
n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate
c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output
h[t] = o[t] .* tanh(c[t])</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/rnn.jl#L250-L292">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.rnnforw" href="#Knet.rnnforw"><code>Knet.rnnforw</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">rnnforw(r, w, x[, hx, cx]; batchSizes, hy, cy)</code></pre><p>Returns a tuple (y,hyout,cyout,rs) given rnn <code>r</code>, weights <code>w</code>, input <code>x</code> and optionally the initial hidden and cell states <code>hx</code> and <code>cx</code> (<code>cx</code> is only used in LSTMs).  <code>r</code> and <code>w</code> should come from a previous call to <code>rnninit</code>.  Both <code>hx</code> and <code>cx</code> are optional, they are treated as zero arrays if not provided.  The output <code>y</code> contains the hidden states of the final layer for each time step, <code>hyout</code> and <code>cyout</code> give the final hidden and cell states for all layers, <code>rs</code> is a buffer the RNN needs for its gradient calculation.</p><p>The boolean keyword arguments <code>hy</code> and <code>cy</code> control whether <code>hyout</code> and <code>cyout</code> will be output.  By default <code>hy = (hx!=nothing)</code> and <code>cy = (cx!=nothing &amp;&amp; r.mode==2)</code>, i.e. a hidden state will be output if one is provided as input and for cell state we also require an LSTM.  If <code>hy</code>/<code>cy</code> is <code>false</code>, <code>hyout</code>/<code>cyout</code> will be <code>nothing</code>. <code>batchSizes</code> can be an integer array that specifies non-uniform batch sizes as explained below. By default <code>batchSizes=nothing</code> and the same batch size, <code>size(x,2)</code>, is used for all time steps.</p><p>The input and output dimensions are:</p><ul><li><p><code>x</code>: (X,[B,T])</p></li><li><p><code>y</code>: (H/2H,[B,T])</p></li><li><p><code>hx</code>,<code>cx</code>,<code>hyout</code>,<code>cyout</code>: (H,B,L/2L)</p></li><li><p><code>batchSizes</code>: <code>nothing</code> or <code>Vector{Int}(T)</code></p></li></ul><p>where X is inputSize, H is hiddenSize, B is batchSize, T is seqLength, L is numLayers.  <code>x</code> can be 1, 2, or 3 dimensional.  If <code>batchSizes==nothing</code>, a 1-D <code>x</code> represents a single instance, a 2-D <code>x</code> represents a single minibatch, and a 3-D <code>x</code> represents a sequence of identically sized minibatches.  If <code>batchSizes</code> is an array of (non-increasing) integers, it gives us the batch size for each time step in the sequence, in which case <code>sum(batchSizes)</code> should equal <code>div(length(x),size(x,1))</code>. <code>y</code> has the same dimensionality as <code>x</code>, differing only in its first dimension, which is H if the RNN is unidirectional, 2H if bidirectional.  Hidden vectors <code>hx</code>, <code>cx</code>, <code>hyout</code>, <code>cyout</code> all have size (H,B1,L) for unidirectional RNNs, and (H,B1,2L) for bidirectional RNNs where B1 is the size of the first minibatch.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/rnn.jl#L342-L385">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.rnnparam" href="#Knet.rnnparam"><code>Knet.rnnparam</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">rnnparam{T}(r::RNN, w::KnetArray{T}, layer, id, param)</code></pre><p>Return a single weight matrix or bias vector as a subarray of w.</p><p>Valid <code>layer</code> values:</p><ul><li><p>For unidirectional RNNs 1:numLayers</p></li><li><p>For bidirectional RNNs 1:2*numLayers, forw and back layers alternate.</p></li></ul><p>Valid <code>id</code> values:</p><ul><li><p>For RELU and TANH RNNs, input = 1, hidden = 2.</p></li><li><p>For GRU reset = 1,4; update = 2,5; newmem = 3,6; 1:3 for input, 4:6 for hidden</p></li><li><p>For LSTM inputgate = 1,5; forget = 2,6; newmem = 3,7; output = 4,8; 1:4 for input, 5:8 for hidden</p></li></ul><p>Valid <code>param</code> values:</p><ul><li><p>Return the weight matrix (transposed!) if <code>param==1</code>.</p></li><li><p>Return the bias vector if <code>param==2</code>.</p></li></ul><p>The effect of skipInput: Let I=1 for RELU/TANH, 1:3 for GRU, 1:4 for LSTM</p><ul><li><p>For skipInput=false (default), rnnparam(r,w,1,I,1) is a (inputSize,hiddenSize) matrix.</p></li><li><p>For skipInput=true, rnnparam(r,w,1,I,1) is <code>nothing</code>.</p></li><li><p>For bidirectional, the same applies to rnnparam(r,w,2,I,1): the first back layer.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/rnn.jl#L152-L176">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.rnnparams" href="#Knet.rnnparams"><code>Knet.rnnparams</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">rnnparams(r::RNN, w)</code></pre><p>Split w into individual parameters and return them as an array.</p><p>The order of params returned (subject to change):</p><ul><li><p>All weight matrices come before all bias vectors.</p></li><li><p>Matrices and biases are sorted lexically based on (layer,id).</p></li><li><p>See @doc rnnparam for valid layer and id values.</p></li><li><p>Input multiplying matrices are <code>nothing</code> if r.inputMode = 1.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/rnn.jl#L222-L234">source</a></section><h2><a class="nav-anchor" id="Optimization-methods-1" href="#Optimization-methods-1">Optimization methods</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.update!" href="#Knet.update!"><code>Knet.update!</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">update!(weights, gradients, params)
update!(weights, gradients; lr=0.001, gclip=0)</code></pre><p>Update the <code>weights</code> using their <code>gradients</code> and the optimization algorithm parameters specified by <code>params</code>.  The 2-arg version defaults to the <a href="reference.html#Knet.Sgd"><code>Sgd</code></a> algorithm with learning rate <code>lr</code> and gradient clip <code>gclip</code>.  <code>gclip==0</code> indicates no clipping. The <code>weights</code> and possibly <code>gradients</code> and <code>params</code> are modified in-place.</p><p><code>weights</code> can be an individual numeric array or a collection of arrays represented by an iterator or dictionary.  In the individual case, <code>gradients</code> should be a similar numeric array of <code>size(weights)</code> and <code>params</code> should be a single object.  In the collection case, each individual weight array should have a corresponding params object. This way different weight arrays can have their own optimization state, different learning rates, or even different optimization algorithms running in parallel.  In the iterator case, <code>gradients</code> and <code>params</code> should be iterators of the same length as <code>weights</code> with corresponding elements.  In the dictionary case, <code>gradients</code> and <code>params</code> should be dictionaries with the same keys as <code>weights</code>.</p><p>Individual optimization parameters can be one of the following types. The keyword arguments for each type&#39;s constructor and their default values are listed as well.</p><ul><li><p><a href="reference.html#Knet.Sgd"><code>Sgd</code></a><code>(;lr=0.001, gclip=0)</code></p></li><li><p><a href="reference.html#Knet.Momentum"><code>Momentum</code></a><code>(;lr=0.001, gclip=0, gamma=0.9)</code></p></li><li><p><a href="reference.html#Knet.Nesterov"><code>Nesterov</code></a><code>(;lr=0.001, gclip=0, gamma=0.9)</code></p></li><li><p><a href="reference.html#Knet.Rmsprop"><code>Rmsprop</code></a><code>(;lr=0.001, gclip=0, rho=0.9, eps=1e-6)</code></p></li><li><p><a href="reference.html#Knet.Adagrad"><code>Adagrad</code></a><code>(;lr=0.1, gclip=0, eps=1e-6)</code></p></li><li><p><a href="reference.html#Knet.Adadelta"><code>Adadelta</code></a><code>(;lr=0.01, gclip=0, rho=0.9, eps=1e-6)</code></p></li><li><p><a href="reference.html#Knet.Adam"><code>Adam</code></a><code>(;lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)</code></p></li></ul><p><strong>Example:</strong></p><pre><code class="language-none">w = rand(d)                 # an individual weight array
g = lossgradient(w)         # gradient g has the same shape as w
update!(w, g)               # update w in-place with Sgd()
update!(w, g; lr=0.1)       # update w in-place with Sgd(lr=0.1)
update!(w, g, Sgd(lr=0.1))  # update w in-place with Sgd(lr=0.1)

w = (rand(d1), rand(d2))    # a tuple of weight arrays
g = lossgradient2(w)        # g will also be a tuple
p = (Adam(), Sgd())         # p has params for each w[i]
update!(w, g, p)            # update each w[i] in-place with g[i],p[i]

w = Any[rand(d1), rand(d2)] # any iterator can be used
g = lossgradient3(w)        # g will be similar to w
p = Any[Adam(), Sgd()]      # p should be an iterator of same length
update!(w, g, p)            # update each w[i] in-place with g[i],p[i]

w = Dict(:a =&gt; rand(d1), :b =&gt; rand(d2)) # dictionaries can be used
g = lossgradient4(w)
p = Dict(:a =&gt; Adam(), :b =&gt; Sgd())
update!(w, g, p)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L288-L345">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.optimizers" href="#Knet.optimizers"><code>Knet.optimizers</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">optimizers(model, otype; options...)</code></pre><p>Given parameters of a <code>model</code>, initialize and return corresponding optimization parameters for a given optimization type <code>otype</code> and optimization options <code>options</code>. This is useful because each numeric array in model needs its own distinct optimization parameter. <code>optimizers</code> makes the creation of optimization parameters that parallel model parameters easy when all of them use the same type and options.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L490-L501">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Adadelta" href="#Knet.Adadelta"><code>Knet.Adadelta</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Adadelta(;lr=0.01, gclip=0, rho=0.9, eps=1e-6)
update!(w,g,p::Adadelta)</code></pre><p>Container for parameters of the Adadelta optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>Adadelta is an extension of Adagrad that tries to prevent the decrease of the learning rates to zero as training progresses. It scales the learning rate based on the accumulated gradients like Adagrad and holds the acceleration term like Momentum. It updates the weights with the following formulas:</p><pre><code class="language-none">G = (1-rho) * g .^ 2 + rho * G
update = g .* sqrt(delta + eps) ./ sqrt(G + eps)
w = w - lr * update
delta = rho * delta + (1-rho) * update .^ 2</code></pre><p>where <code>w</code> is the weight, <code>g</code> is the gradient of the objective function w.r.t <code>w</code>, <code>lr</code> is the learning rate, <code>G</code> is an array with the same size and type of <code>w</code> and holds the sum of the squares of the gradients. <code>eps</code> is a small constant to prevent a zero value in the denominator.  <code>rho</code> is the momentum parameter and <code>delta</code> is an array with the same size and type of <code>w</code> and holds the sum of the squared updates.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip==0</code> no scaling takes place.</p><p>Reference: <a href="http://arxiv.org/abs/1212.5701">Zeiler, M. D. (2012)</a>. ADADELTA: An Adaptive Learning Rate Method.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L152-L185">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Adagrad" href="#Knet.Adagrad"><code>Knet.Adagrad</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Adagrad(;lr=0.1, gclip=0, eps=1e-6)
update!(w,g,p::Adagrad)</code></pre><p>Container for parameters of the Adagrad optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>Adagrad is one of the methods that adapts the learning rate to each of the weights.  It stores the sum of the squares of the gradients to scale the learning rate.  The learning rate is adapted for each weight by the value of current gradient divided by the accumulated gradients. Hence, the learning rate is greater for the parameters where the accumulated gradients are small and the learning rate is small if the accumulated gradients are large. It updates the weights with the following formulas:</p><pre><code class="language-none">G = G + g .^ 2
w = w - g .* lr ./ sqrt(G + eps)</code></pre><p>where <code>w</code> is the weight, <code>g</code> is the gradient of the objective function w.r.t <code>w</code>, <code>lr</code> is the learning rate, <code>G</code> is an array with the same size and type of <code>w</code> and holds the sum of the squares of the gradients. <code>eps</code> is a small constant to prevent a zero value in the denominator.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip==0</code> no scaling takes place.</p><p>Reference: <a href="http://jmlr.org/papers/v12/duchi11a.html">Duchi, J., Hazan, E., &amp; Singer, Y. (2011)</a>. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L108-L141">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Adam" href="#Knet.Adam"><code>Knet.Adam</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Adam(;lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)
update!(w,g,p::Adam)</code></pre><p>Container for parameters of the Adam optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>Adam is one of the methods that compute the adaptive learning rate. It stores accumulated gradients (first moment) and the sum of the squared of gradients (second).  It scales the first and second moment as a function of time. Here is the update formulas:</p><pre><code class="language-none">m = beta1 * m + (1 - beta1) * g
v = beta2 * v + (1 - beta2) * g .* g
mhat = m ./ (1 - beta1 ^ t)
vhat = v ./ (1 - beta2 ^ t)
w = w - (lr / (sqrt(vhat) + eps)) * mhat</code></pre><p>where <code>w</code> is the weight, <code>g</code> is the gradient of the objective function w.r.t <code>w</code>, <code>lr</code> is the learning rate, <code>m</code> is an array with the same size and type of <code>w</code> and holds the accumulated gradients. <code>v</code> is an array with the same size and type of <code>w</code> and holds the sum of the squares of the gradients. <code>eps</code> is a small constant to prevent a zero denominator. <code>beta1</code> and <code>beta2</code> are the parameters to calculate bias corrected first and second moments. <code>t</code> is the update count.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip==0</code> no scaling takes place.</p><p>Reference: <a href="https://arxiv.org/abs/1412.6980">Kingma, D. P., &amp; Ba, J. L. (2015)</a>. Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L239-L273">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Momentum" href="#Knet.Momentum"><code>Knet.Momentum</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Momentum(;lr=0.001, gclip=0, gamma=0.9)
update!(w,g,p::Momentum)</code></pre><p>Container for parameters of the Momentum optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>The Momentum method tries to accelerate SGD by adding a velocity term to the update.  This also decreases the oscillation between successive steps. It updates the weights with the following formulas:</p><pre><code class="language-none">velocity = gamma * velocity + lr * g
w = w - velocity</code></pre><p>where <code>w</code> is a weight array, <code>g</code> is the gradient of the objective function w.r.t <code>w</code>, <code>lr</code> is the learning rate, <code>gamma</code> is the momentum parameter, <code>velocity</code> is an array with the same size and type of <code>w</code> and holds the accelerated gradients.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip==0</code> no scaling takes place.</p><p>Reference: <a href="http://doi.org/10.1016/S0893-6080(98)00116-6">Qian, N. (1999)</a>. On the momentum term in gradient descent learning algorithms.  Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L35-L63">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Nesterov" href="#Knet.Nesterov"><code>Knet.Nesterov</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Nesterov(; lr=0.001, gclip=0, gamma=0.9)
update!(w,g,p::Momentum)</code></pre><p>Container for parameters of Nesterov&#39;s momentum optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>It is similar to standard <a href="reference.html#Knet.Momentum"><code>Momentum</code></a> but with a slightly different update rule:</p><pre><code class="language-none">velocity = gamma * velocity_old - lr * g
w = w_old - velocity_old + (1+gamma) * velocity</code></pre><p>where <code>w</code> is a weight array, <code>g</code> is the gradient of the objective function w.r.t <code>w</code>, <code>lr</code> is the learning rate, <code>gamma</code> is the momentum parameter, <code>velocity</code> is an array with the same size and type of <code>w</code> and holds the accelerated gradients.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip == 0</code> no scaling takes place.</p><p>Reference Implementation : <a href="https://arxiv.org/pdf/1212.0901.pdf">Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan P ascanu</a></p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L74-L97">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Rmsprop" href="#Knet.Rmsprop"><code>Knet.Rmsprop</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Rmsprop(;lr=0.001, gclip=0, rho=0.9, eps=1e-6)
update!(w,g,p::Rmsprop)</code></pre><p>Container for parameters of the Rmsprop optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>Rmsprop scales the learning rates by dividing the root mean squared of the gradients. It updates the weights with the following formula:</p><pre><code class="language-none">G = (1-rho) * g .^ 2 + rho * G
w = w - lr * g ./ sqrt(G + eps)</code></pre><p>where <code>w</code> is the weight, <code>g</code> is the gradient of the objective function w.r.t <code>w</code>, <code>lr</code> is the learning rate, <code>G</code> is an array with the same size and type of <code>w</code> and holds the sum of the squares of the gradients. <code>eps</code> is a small constant to prevent a zero value in the denominator.  <code>rho</code> is the momentum parameter and <code>delta</code> is an array with the same size and type of <code>w</code> and holds the sum of the squared updates.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip==0</code> no scaling takes place.</p><p>Reference: <a href="https://dirtysalt.github.io/images/nn-class-lec6.pdf">Tijmen Tieleman and Geoffrey Hinton (2012)</a>. &quot;Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.&quot;  COURSERA: Neural Networks for Machine Learning 4.2.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L198-L227">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.Sgd" href="#Knet.Sgd"><code>Knet.Sgd</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">Sgd(;lr=0.001,gclip=0)
update!(w,g,p::Sgd)
update!(w,g;lr=0.001)</code></pre><p>Container for parameters of the Stochastic gradient descent (SGD) optimization algorithm used by <a href="reference.html#Knet.update!"><code>update!</code></a>.</p><p>SGD is an optimization technique to minimize an objective function by updating its weights in the opposite direction of their gradient. The learning rate (lr) determines the size of the step.  SGD updates the weights with the following formula:</p><pre><code class="language-none">w = w - lr * g</code></pre><p>where <code>w</code> is a weight array, <code>g</code> is the gradient of the loss function w.r.t <code>w</code> and <code>lr</code> is the learning rate.</p><p>If <code>vecnorm(g) &gt; gclip &gt; 0</code>, <code>g</code> is scaled so that its norm is equal to <code>gclip</code>.  If <code>gclip==0</code> no scaling takes place.</p><p>SGD is used by default if no algorithm is specified in the two argument version of <code>update!</code>[@ref].</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/update.jl#L1-L24">source</a></section><h2><a class="nav-anchor" id="Hyperparameter-optimization-1" href="#Hyperparameter-optimization-1">Hyperparameter optimization</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.goldensection" href="#Knet.goldensection"><code>Knet.goldensection</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">goldensection(f,n;kwargs) =&gt; (fmin,xmin)</code></pre><p>Find the minimum of <code>f</code> using concurrent golden section search in <code>n</code> dimensions. See <code>Knet.goldensection_demo()</code> for an example.</p><p><code>f</code> is a function from a <code>Vector{Float64}</code> of length <code>n</code> to a <code>Number</code>.  It can return <code>NaN</code> for out of range inputs.  Goldensection will always start with a zero vector as the initial input to <code>f</code>, and the initial step size will be 1 in each dimension.  The user should define <code>f</code> to scale and shift this input range into a vector meaningful for their application. For positive inputs like learning rate or hidden size, you can use a transformation such as <code>x0*exp(x)</code> where <code>x</code> is a value <code>goldensection</code> passes to <code>f</code> and <code>x0</code> is your initial guess for this value. This will effectively start the search at <code>x0</code>, then move with multiplicative steps.</p><p>I designed this algorithm combining ideas from <a href="http://apps.nrbook.com/empanel/index.html?pg=492">Golden Section Search</a> and <a href="https://en.wikipedia.org/wiki/Hill_climbing">Hill Climbing Search</a>. It essentially runs golden section search concurrently in each dimension, picking the next step based on estimated gain.</p><p><strong>Keyword arguments</strong></p><ul><li><p><code>dxmin=0.1</code>: smallest step size.</p></li><li><p><code>accel=φ</code>: acceleration rate. Golden ratio <code>φ=1.618...</code> is best.</p></li><li><p><code>verbose=false</code>: use <code>true</code> to print individual steps.</p></li><li><p><code>history=[]</code>: cache of <code>[(x,f(x)),...]</code> function evaluations.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/hyperopt.jl#L1-L31">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.hyperband" href="#Knet.hyperband"><code>Knet.hyperband</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">hyperband(getconfig, getloss, maxresource=27, reduction=3)</code></pre><p>Hyperparameter optimization using the hyperband algorithm from (<a href="https://arxiv.org/abs/1603.06560">Lisha et al. 2016</a>).  You can try a simple MNIST example using <code>Knet.hyperband_demo()</code>. </p><p><strong>Arguments</strong></p><ul><li><p><code>getconfig()</code> returns random configurations with a user defined type and distribution.</p></li><li><p><code>getloss(c,n)</code> returns loss for configuration <code>c</code> and number of resources (e.g. epochs) <code>n</code>.</p></li><li><p><code>maxresource</code> is the maximum number of resources any one configuration should be given.</p></li><li><p><code>reduction</code> is an algorithm parameter (see paper), 3 is a good value.</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/hyperopt.jl#L109-L123">source</a></section><h2><a class="nav-anchor" id="Initialization-1" href="#Initialization-1">Initialization</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.bilinear" href="#Knet.bilinear"><code>Knet.bilinear</code></a> — <span class="docstring-category">Function</span>.</div><div><p>Bilinear interpolation filter weights; used for initializing deconvolution layers.</p><p>Adapted from https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py#L33</p><p>Arguments:</p><p><code>T</code> : Data Type</p><p><code>fw</code>: Width upscale factor</p><p><code>fh</code>: Height upscale factor</p><p><code>IN</code>: Number of input filters</p><p><code>ON</code>: Number of output filters</p><p>Example usage:</p><p>w = bilinear(Float32,2,2,128,128)</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/distributions.jl#L44-L67">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.gaussian" href="#Knet.gaussian"><code>Knet.gaussian</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">gaussian(a...; mean=0.0, std=0.01)</code></pre><p>Return a Gaussian array with a given mean and standard deviation.  The <code>a</code> arguments are passed to <code>randn</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/distributions.jl#L1-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Knet.xavier" href="#Knet.xavier"><code>Knet.xavier</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">xavier(a...)</code></pre><p>Xavier initialization.  The <code>a</code> arguments are passed to <code>rand</code>.  See (<a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot and Bengio 2010</a>) for a description. <a href="http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1XavierFiller.html#details">Caffe</a> implements this slightly differently. <a href="http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform">Lasagne</a> calls it <code>GlorotUniform</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/Knet.jl/blob/49e6d18d9aed3cc785763c3e537d3553ea4db16a/src/distributions.jl#L15-L27">source</a></section><h2><a class="nav-anchor" id="AutoGrad-(advanced)-1" href="#AutoGrad-(advanced)-1">AutoGrad (advanced)</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.getval" href="#AutoGrad.getval"><code>AutoGrad.getval</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">getval(x)</code></pre><p>Unbox <code>x</code> if it is a boxed value (<code>Rec</code>), otherwise return <code>x</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/blob/0c27dd85875a8c0cdbc9a4dd11ffbf72493ecbc2/src/core.jl#L173-L179">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@primitive" href="#AutoGrad.@primitive"><code>AutoGrad.@primitive</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">@primitive fx g1 g2...</code></pre><p>Define a new primitive operation for AutoGrad and (optionally) specify its gradients.  Non-differentiable functions such as <code>sign</code>, and non-numeric functions such as <code>size</code> should be defined using the @zerograd macro instead.</p><p><strong>Examples</strong></p><pre><code class="language-none">@primitive sin(x::Number)
@primitive hypot(x1::Array,x2::Array),dy,y

@primitive sin(x::Number),dy  (dy*cos(x))
@primitive hypot(x1::Array,x2::Array),dy,y  (dy.*x1./y)  (dy.*x2./y)</code></pre><p>The first example shows that <code>fx</code> is a typed method declaration. Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types.  AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.</p><p>The second example specifies variable names for the output gradient <code>dy</code> and the output <code>y</code> after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in <code>f(a::Int,b,c...;d=1)</code>.  Parametric methods such as <code>f{T&lt;:Number}(x::T)</code> cannot be used.</p><p>The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.</p><p><strong>Under the hood</strong></p><p>The @primitive macro turns the first example into:</p><pre><code class="language-none">let sin_r = recorder(sin)
    global sin
    sin{T&lt;:Number}(x::Rec{T}) = sin_r(x)
end</code></pre><p>This will cause calls to <code>sin</code> with a boxed argument (<code>Rec{T&lt;:Number}</code>) to be recorded.  The recorded operations are used by <code>grad</code> to construct a dynamic computational graph.  With multiple arguments things are a bit more complicated.  Here is what happens with the second example:</p><pre><code class="language-none">let hypot_r = recorder(hypot)
    global hypot
    hypot{T&lt;:Array,S&lt;:Array}(x1::Rec{T},x2::Rec{S})=hypot_r(x1,x2)
    hypot{T&lt;:Array,S&lt;:Array}(x1::Rec{T},x2::S)=hypot_r(x1,x2)
    hypot{T&lt;:Array,S&lt;:Array}(x1::T,x2::Rec{S})=hypot_r(x1,x2)
end</code></pre><p>We want the recorder version to be called if any one of the arguments is a boxed <code>Rec</code>.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.</p><p>In AutoGrad, gradients are defined using gradient methods that have the following signature:</p><pre><code class="language-none">f(Grad{i},dy,y,x...) =&gt; dx[i]</code></pre><p>For the third example here is the generated gradient method:</p><pre><code class="language-none">sin{T&lt;:Number}(::Type{Grad{1}}, dy, y, x::Rec{T})=(dy*cos(x))</code></pre><p>For the last example a different gradient method is generated for each argument:</p><pre><code class="language-none">hypot{T&lt;:Array,S&lt;:Array}(::Type{Grad{1}},dy,y,x1::Rec{T},x2::Rec{S})=(dy.*x1./y)
hypot{T&lt;:Array,S&lt;:Array}(::Type{Grad{2}},dy,y,x1::Rec{T},x2::Rec{S})=(dy.*x2./y)</code></pre><p>In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/blob/0c27dd85875a8c0cdbc9a4dd11ffbf72493ecbc2/src/util.jl#L8-L86">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AutoGrad.@zerograd" href="#AutoGrad.@zerograd"><code>AutoGrad.@zerograd</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">@zerograd f(args...; kwargs...)</code></pre><p>Define <code>f</code> as an AutoGrad primitive operation with zero gradient.</p><p><strong>Example:</strong></p><pre><code class="language-none">@zerograd floor(x::Float32)</code></pre><p><code>@zerograd</code> allows <code>f</code> to handle boxed <code>Rec</code> inputs by unboxing them like a <code>@primitive</code>, but unlike <code>@primitive</code> it does not record its actions or return a boxed <code>Rec</code> result.  Some functions, like <code>sign()</code>, have zero gradient.  Others, like <code>length()</code> have discrete or constant outputs.  These need to handle <code>Rec</code> inputs, but do not need to record anything and can return regular values.  Their output can be treated like a constant in the program.  Use the <code>@zerograd</code> macro for those.  Note that <code>kwargs</code> are NOT unboxed.</p></div><a class="source-link" target="_blank" href="https://github.com/denizyuret/AutoGrad.jl/blob/0c27dd85875a8c0cdbc9a4dd11ffbf72493ecbc2/src/util.jl#L119-L138">source</a></section><h2><a class="nav-anchor" id="Function-Index-1" href="#Function-Index-1">Function Index</a></h2><ul><li><a href="reference.html#Knet.Adadelta"><code>Knet.Adadelta</code></a></li><li><a href="reference.html#Knet.Adagrad"><code>Knet.Adagrad</code></a></li><li><a href="reference.html#Knet.Adam"><code>Knet.Adam</code></a></li><li><a href="reference.html#Knet.KnetArray"><code>Knet.KnetArray</code></a></li><li><a href="reference.html#Knet.Momentum"><code>Knet.Momentum</code></a></li><li><a href="reference.html#Knet.Nesterov"><code>Knet.Nesterov</code></a></li><li><a href="reference.html#Knet.Rmsprop"><code>Knet.Rmsprop</code></a></li><li><a href="reference.html#Knet.Sgd"><code>Knet.Sgd</code></a></li><li><a href="reference.html#AutoGrad.getval"><code>AutoGrad.getval</code></a></li><li><a href="reference.html#AutoGrad.grad"><code>AutoGrad.grad</code></a></li><li><a href="reference.html#AutoGrad.gradcheck"><code>AutoGrad.gradcheck</code></a></li><li><a href="reference.html#AutoGrad.gradloss"><code>AutoGrad.gradloss</code></a></li><li><a href="reference.html#Knet.accuracy"><code>Knet.accuracy</code></a></li><li><a href="reference.html#Knet.bilinear"><code>Knet.bilinear</code></a></li><li><a href="reference.html#Knet.conv4"><code>Knet.conv4</code></a></li><li><a href="reference.html#Knet.deconv4"><code>Knet.deconv4</code></a></li><li><a href="reference.html#Knet.dir"><code>Knet.dir</code></a></li><li><a href="reference.html#Knet.dropout"><code>Knet.dropout</code></a></li><li><a href="reference.html#Knet.gaussian"><code>Knet.gaussian</code></a></li><li><a href="reference.html#Knet.goldensection"><code>Knet.goldensection</code></a></li><li><a href="reference.html#Knet.gpu"><code>Knet.gpu</code></a></li><li><a href="reference.html#Knet.hyperband"><code>Knet.hyperband</code></a></li><li><a href="reference.html#Knet.invx"><code>Knet.invx</code></a></li><li><a href="reference.html#Knet.knetgc"><code>Knet.knetgc</code></a></li><li><a href="reference.html#Knet.logp"><code>Knet.logp</code></a></li><li><a href="reference.html#Knet.logsumexp"><code>Knet.logsumexp</code></a></li><li><a href="reference.html#Knet.mat"><code>Knet.mat</code></a></li><li><a href="reference.html#Knet.minibatch"><code>Knet.minibatch</code></a></li><li><a href="reference.html#Knet.nll"><code>Knet.nll</code></a></li><li><a href="reference.html#Knet.optimizers"><code>Knet.optimizers</code></a></li><li><a href="reference.html#Knet.pool"><code>Knet.pool</code></a></li><li><a href="reference.html#Knet.relu"><code>Knet.relu</code></a></li><li><a href="reference.html#Knet.rnnforw"><code>Knet.rnnforw</code></a></li><li><a href="reference.html#Knet.rnninit"><code>Knet.rnninit</code></a></li><li><a href="reference.html#Knet.rnnparam"><code>Knet.rnnparam</code></a></li><li><a href="reference.html#Knet.rnnparams"><code>Knet.rnnparams</code></a></li><li><a href="reference.html#Knet.setseed"><code>Knet.setseed</code></a></li><li><a href="reference.html#Knet.sigm"><code>Knet.sigm</code></a></li><li><a href="reference.html#Knet.unpool"><code>Knet.unpool</code></a></li><li><a href="reference.html#Knet.update!"><code>Knet.update!</code></a></li><li><a href="reference.html#Knet.xavier"><code>Knet.xavier</code></a></li><li><a href="reference.html#AutoGrad.@primitive"><code>AutoGrad.@primitive</code></a></li><li><a href="reference.html#AutoGrad.@zerograd"><code>AutoGrad.@zerograd</code></a></li></ul><footer><hr/><a class="previous" href="tutorial.html"><span class="direction">Previous</span><span class="title">Introduction to Knet</span></a><a class="next" href="backprop.html"><span class="direction">Next</span><span class="title">Backpropagation</span></a></footer></article></body></html>
