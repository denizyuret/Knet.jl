{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Knet, Plots, JLD, NBInclude\n",
    "@nbinclude(\"02.mnist.ipynb\")  # loads MNIST, defines dtrn,dtst,Atype,train,softmax,zeroone\n",
    "ENV[\"COLUMNS\"]=80         # column width for array printing\n",
    "Plots.plotlyjs()               # for interactive plots\n",
    "Plots.scalefontsizes(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The predict function returns a score for each class as a linear function of input x\n",
    "function linear(w,x)\n",
    "    y = w[1]*mat(x) .+ w[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize weights as a list of [ weightMatrix, biasVector ]\n",
    "winit1(;std=0.01)=map(Atype, [ std*randn(10,784), zeros(10,1) ]);\n",
    "w = winit1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy and zero-one loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Grab a minibatch\n",
    "x,y = first(dtst)\n",
    "map(summary,(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize random model and calculate predictions for one minibatch\n",
    "setseed(9)           # for replicability\n",
    "w = winit1()         # random weight matrix and a zero bias vector\n",
    "ypred = linear(w,x)  # predict\n",
    "Array(ypred)         # predictions are given as a 10xN score matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y'  # correct answers are given as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# accuracy gives percentage of correct answers\n",
    "accuracy(ypred,y)        # 2-arg version: accuracy on this batch of 100 with initial w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy(w,dtst,linear)  # 3-arg version: accuracy on the whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# zeroone loss (error) defined as 1 - accuracy\n",
    "zeroone(w,data,model) = 1 - accuracy(w,data,model)\n",
    "zeroone(w,dtst,linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate softmax (cross entropy, negative log likelihood) loss of a model with weights w for one minibatch (x,y)\n",
    "# Predict specifies the function for model output: ypred = predict(w,x;o...)\n",
    "softmax(w,x,y,predict; o...)=nll(predict(w,x;o...),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# per-instance average softloss for the first test minibatch, should be close to -log(1/10)=2.3\n",
    "softmax(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Manual loss calculation\n",
    "ypred=linear(w,x)\n",
    "yp1 = exp.(ypred)\n",
    "yp2 = yp1 ./ sum(yp1,1)\n",
    "yp3 = -log.(yp2)\n",
    "yc1 = full(sparse(y,1:100,1f0))\n",
    "sum(Array(yp3).*yc1) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "softmax(w,data,predict) = mean(softmax(w,x,y,predict) for (x,y) in data)\n",
    "softmax(w,dtst,linear)  # per-instance average softmax loss for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the gradient using Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Automatically defined gradient for softloss\n",
    "softgrad = grad(softmax)  # softgrad returns gradient wrt 1st arg w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "setseed(9)\n",
    "w1 = winit1(std=0.1)  # use a larger std to get a larger gradient for this example\n",
    "map(size, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g1 = softgrad(w1,x,y,linear)  # gradient has the same size and shape as the first parameter\n",
    "map(size, g1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the gradient using numerical approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Array(g1[2])'  \n",
    "# Meaning of gradient:\n",
    "# If I move the last entry of w[2] by epsilon, loss will go up by 0.345075 epsilon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Array(w1[2])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "softmax(w1,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1[2][10] = 0.1   # to numerically check the gradient let's move the last entry by +0.1.\n",
    "Array(w1[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "softmax(w1,x,y,linear)  \n",
    "# We see that the loss moves by +0.03 as expected.\n",
    "# You should check all/most entries in your gradients this way to make sure they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the gradient using manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Manually defined gradient for softloss\n",
    "function softgrad_manual(w,x,y,predict)\n",
    "    x = mat(x)\n",
    "    p = predict(w,x)\n",
    "    p = p .- maximum(p,1) # for numerical stability\n",
    "    expp = exp.(p)\n",
    "    p = expp ./ sum(expp,1)\n",
    "    q = oftype(p, sparse(convert(Vector{Int},y),1:length(y),1,size(p,1),length(y)))\n",
    "    dJdy = (p - q) / size(x,2)\n",
    "    dJdw = dJdy * x'\n",
    "    dJdb = sum(dJdy,2)\n",
    "    Any[dJdw,dJdb]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "g2 = softgrad_manual(w1,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "isapprox(g1[1],g2[1],rtol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "isapprox(g1[2],g2[2],rtol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (SGD) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train model(w) with SGD and return a list containing w for every epoch\n",
    "function train(w,data,predict; epochs=100,lr=0.1,o...)\n",
    "    weights = Any[deepcopy(w)]\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = softgrad(w,x,y,predict;o...)\n",
    "            update!(w,g,lr=lr)  # w[i] = w[i] - lr * g[i]\n",
    "        end\n",
    "        push!(weights,deepcopy(w))\n",
    "    end\n",
    "    return weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the linear model and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if !isfile(\"lin.jld\")\n",
    "    setseed(1)\n",
    "    @time weights = train(winit1(),dtrn,linear,lr=0.1)           # 31.1s\n",
    "    @time trnloss = [ softmax(w,dtrn,linear) for w in weights ]  # 22.2s\n",
    "    @time tstloss = [ softmax(w,dtst,linear) for w in weights ]  # 3.7s\n",
    "    @time trnerr  = [ zeroone(w,dtrn,linear) for w in weights ]  # 20.6s\n",
    "    @time tsterr  = [ zeroone(w,dtst,linear) for w in weights ]  # 3.4s\n",
    "    @save \"lin.jld\" weights trnloss tstloss trnerr tsterr\n",
    "else\n",
    "    @eval (@load \"lin.jld\")\n",
    "end\n",
    "minimum(tstloss),minimum(tsterr)  # 0.2667, 0.0744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot([trnloss tstloss],ylim=(.2,.36),labels=[:trnloss :tstloss],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# Demonstrates underfitting: training loss not close to 0\n",
    "# Also slight overfitting: test loss higher than train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot([trnerr tsterr],ylim=(.06,.10),labels=[:trnerr :tsterr],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# this is the error plot, we get to about 7.5% test error, i.e. 92.5% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for t in logspace(0,2,20)\n",
    "    i = ceil(Int,t)\n",
    "    w = weights[i]\n",
    "    w1 = reshape(Array(w[1])', (28,28,1,10))\n",
    "    w2 = clamp.(w1.+0.5,0,1)\n",
    "    IJulia.clear_output(true)\n",
    "    display(hcat([mnistview(w2,i) for i=1:10]...))\n",
    "    display(\"Epoch $i\")\n",
    "    sleep(1) # (0.96^i)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
