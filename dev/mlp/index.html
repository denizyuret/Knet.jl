<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multilayer Perceptrons · Knet.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Knet.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../install/">Setting up Knet</a></li><li><a class="tocitem" href="../tutorial/">Introduction to Knet</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul></li><li><span class="tocitem">Textbook</span><ul><li><a class="tocitem" href="../backprop/">Backpropagation and SGD</a></li><li><a class="tocitem" href="../softmax/">Softmax Classification</a></li><li class="is-active"><a class="tocitem" href>Multilayer Perceptrons</a><ul class="internal"><li><a class="tocitem" href="#Stacking-linear-classifiers-is-useless"><span>Stacking linear classifiers is useless</span></a></li><li><a class="tocitem" href="#Introducing-nonlinearities"><span>Introducing nonlinearities</span></a></li><li><a class="tocitem" href="#Types-of-nonlinearities-(activation-functions)"><span>Types of nonlinearities (activation functions)</span></a></li><li><a class="tocitem" href="#Representational-power"><span>Representational power</span></a></li><li><a class="tocitem" href="#Matrix-vs-Neuron-Pictures"><span>Matrix vs Neuron Pictures</span></a></li><li><a class="tocitem" href="#Programming-Example"><span>Programming Example</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../cnn/">Convolutional Neural Networks</a></li><li><a class="tocitem" href="../rnn/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../rl/">Reinforcement Learning</a></li><li><a class="tocitem" href="../opt/">Optimization</a></li><li><a class="tocitem" href="../gen/">Generalization</a></li><li><a class="tocitem" href="../nce/">Noise Contrastive Estimation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Textbook</a></li><li class="is-active"><a href>Multilayer Perceptrons</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multilayer Perceptrons</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/mlp.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Multilayer-Perceptrons"><a class="docs-heading-anchor" href="#Multilayer-Perceptrons">Multilayer Perceptrons</a><a id="Multilayer-Perceptrons-1"></a><a class="docs-heading-anchor-permalink" href="#Multilayer-Perceptrons" title="Permalink"></a></h1><p>In this section we create multilayer perceptrons by stacking multiple linear layers with non-linear activation functions in between.</p><h2 id="Stacking-linear-classifiers-is-useless"><a class="docs-heading-anchor" href="#Stacking-linear-classifiers-is-useless">Stacking linear classifiers is useless</a><a id="Stacking-linear-classifiers-is-useless-1"></a><a class="docs-heading-anchor-permalink" href="#Stacking-linear-classifiers-is-useless" title="Permalink"></a></h2><p>We could try stacking multiple linear classifiers together. Here is a two layer model:</p><pre><code class="language-none">function multilinear(w, x, ygold)
    y1 = w[1] * x  .+ w[2]
    y2 = w[3] * y1 .+ w[4]
    return softloss(ygold, y2)
end</code></pre><p>Note that instead of using <code>y1</code> as our prediction, we used it as input to another linear classifier. Intermediate arrays like <code>y1</code> are known as <strong>hidden layers</strong> because their contents are not directly visible outside the model.</p><p>If you experiment with this model (I suggest using a smaller learning rate, e.g. 0.01), you will see that it performs similarly to the original softmax model. The reason is simple to see if we write the function computed in mathematical notation and do some algebra:</p><p class="math-container">\[\begin{aligned}
\hat{p} &amp;= \operatorname{softmax}(W_2 (W_1 x + b_1) + b_2) \\
&amp;= \operatorname{softmax}((W_2 W_1)\, x + W_2 b_1 + b_2) \\
&amp;= \operatorname{softmax}(W x + b)
\end{aligned}\]</p><p>where <span>$W=W_2 W_1$</span> and <span>$b=W_2 b_1 + b_2$</span>. In other words, we still have a linear classifier! No matter how many linear functions you put on top of each other, what you get at the end is still a linear function. So this model has exactly the same representation power as the softmax model. Unless, we add a simple instruction...</p><h2 id="Introducing-nonlinearities"><a class="docs-heading-anchor" href="#Introducing-nonlinearities">Introducing nonlinearities</a><a id="Introducing-nonlinearities-1"></a><a class="docs-heading-anchor-permalink" href="#Introducing-nonlinearities" title="Permalink"></a></h2><p>Here is a slightly modified version of the two layer model:</p><pre><code class="language-none">function mlp(w, x, ygold)
    y1 = relu(w[1] * x .+ w[2])
    y2 = w[3] * y1 .+ w[4]
    return softloss(ygold, y2)
end</code></pre><p>MLP in <code>mlp</code> stands for <strong>multilayer perceptron</strong> which is one name for this type of model. The only difference with the previous example is the <code>relu()</code> function we introduced in the first line. This is known as the rectified linear unit (or rectifier), and is a simple function defined by <code>relu(x)=max(x,0)</code> applied elementwise to the input array. So mathematically what we are computing is:</p><p class="math-container">\[\hat{p} = \operatorname{softmax}(W_2\, \operatorname{relu}(W_1 x + b_1) + b_2) \\\]</p><p>This cannot be reduced to a linear function, which may not seem like a big difference but what a difference it makes to the model! Here are the learning curves for <code>mlp</code> using a hidden layer of size 64:</p><blockquote><p><img src="../images/mnist_mlp.png" alt="image"/></p></blockquote><p>Here are the learning curves for the linear model <code>softmax</code> plotted at the same scale for comparison:</p><blockquote><p><img src="../images/mnist_softmax2.png" alt="image"/></p></blockquote><p>We can observe a few things: using MLP instead of a linear model brings the training error from 6.7% to 0 and the test error from 7.5% to 2.0%. There is still overfitting: the test error is not as good as the training error, but the model has no problem classifying the training data (all 60,000 examples) perfectly!</p><h2 id="Types-of-nonlinearities-(activation-functions)"><a class="docs-heading-anchor" href="#Types-of-nonlinearities-(activation-functions)">Types of nonlinearities (activation functions)</a><a id="Types-of-nonlinearities-(activation-functions)-1"></a><a class="docs-heading-anchor-permalink" href="#Types-of-nonlinearities-(activation-functions)" title="Permalink"></a></h2><p>The functions we throw between linear layers to break the linearity are called <strong>nonlinearities</strong> or <strong>activation functions</strong>. Here are some activation functions that have been used as nonlinearities:</p><blockquote><p><img src="../images/actf.png" alt="image"/></p></blockquote><p>The step functions were the earliest activation functions used in the perceptrons of 1950s. Unfortunately they do not give a useful derivative that can be used for training a multilayer model. Sigmoid and tanh (<code>sigm</code> and <code>tanh</code> in Knet) became popular in 1980s as smooth approximations to the step functions and allowed the application of the backpropagation algorithm. Modern activation functions like relu and maxout are piecewise linear. They are computationally inexpensive (no exponentials), and perform well in practice. We are going to use relu in most of our models. Here is the backward passes for sigmoid, tanh, and relu:</p><table><tr><th style="text-align: right">function</th><th style="text-align: right">forward</th><th style="text-align: right">backward</th></tr><tr><td style="text-align: right">sigmoid</td><td style="text-align: right"><span>$y = \frac{1}{1+e^{-x}}$</span></td><td style="text-align: right"><span>$\nabla_x J = y\,(1-y) \nabla_y J$</span></td></tr><tr><td style="text-align: right">tanh</td><td style="text-align: right"><span>$y = \frac{e^x-e^{-x}}{e^x+e^{-x}}$</span></td><td style="text-align: right"><span>$\nabla_x J = (1+y)(1-y) \nabla_y J$</span></td></tr><tr><td style="text-align: right">relu</td><td style="text-align: right"><span>$y = \max(0,x)$</span></td><td style="text-align: right"><span>$\nabla_x J = [ y \geq 0 ] \nabla_y J$</span></td></tr></table><p>See <a href="http://cs231n.github.io/neural-networks-1">(Karpathy, 2016, Ch 1)</a> for more on activation functions and MLP architecture.</p><h2 id="Representational-power"><a class="docs-heading-anchor" href="#Representational-power">Representational power</a><a id="Representational-power-1"></a><a class="docs-heading-anchor-permalink" href="#Representational-power" title="Permalink"></a></h2><p>You might be wondering whether <code>relu</code> had any special properties or would any of the other nonlinearities be sufficient. Another question is whether there are functions multilayer perceptrons cannot represent and if so whether adding more layers or different types of functions would increase their representational power. The short answer is that a two layer model can approximate any function if the hidden layer is large enough, and can do so with any of the nonlinearities introduced in the last section. Multilayer perceptrons are universal function approximators!</p><p>We said that a two-layer MLP is a universal function approximator <em>given enough hidden units</em>. This brings up the questions of efficiency: how many hidden units / parameters does one need to approximate a given function and whether the number of units depends on the number of hidden layers. The efficiency is important both computationally and statistically: models with fewer parameters can be evaluated faster, and can learn from fewer examples (ref?). It turns out there are functions whose representations are <em>exponentially more expensive</em> in a shallow network compared to a deeper network (see <a href="http://neuralnetworksanddeeplearning.com/chap5.html">(Nielsen, 2016, Ch 5)</a> for a discussion). Recent winners of image recognition contests use networks with dozens of convolutional layers. The advantage of deeper MLPs is empirically less clear, but you should experiment with the number of units and layers using a development set when starting a new problem.</p><p>Please see <a href="http://neuralnetworksanddeeplearning.com/chap4.html">(Nielsen, 2016, Ch 4)</a> for an intuitive explanation of the universality result and <a href="http://www.deeplearningbook.org/contents/mlp.html">(Bengio et al. 2016, Ch 6.4)</a> for a more in depth discussion and references.</p><h2 id="Matrix-vs-Neuron-Pictures"><a class="docs-heading-anchor" href="#Matrix-vs-Neuron-Pictures">Matrix vs Neuron Pictures</a><a id="Matrix-vs-Neuron-Pictures-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-vs-Neuron-Pictures" title="Permalink"></a></h2><p>So far we have introduced multilayer perceptrons (aka artificial neural networks) using matrix operations. You may be wondering why people call them neural networks and be confused by terms like layers and units. In this section we will give the correspondence between the matrix view and the neuron view. Here is a schematic of a biological neuron (figures from <a href="http://cs231n.github.io/neural-networks-1">(Karpathy, 2016, Ch 1)</a>):</p><blockquote><p><img src="../images/neuron.png" alt="image"/></p></blockquote><p>A biological neuron is a complex organism supporting thousands of chemical reactions simultaneously under the regulation of thousands of genes, communicating with other neurons through electrical and chemical pathways involving dozens of different types of neurotransmitter molecules. We assume (do not know for sure) that the main mechanism of communication between neurons is electrical spike trains that travel from the axon of the source neuron, through connections called synapses, into dendrites of target neurons. We simplify this picture further representing the strength of the spikes and the connections with simple numbers to arrive at this cartoon model:</p><p><img src="../images/neuron_model.jpeg" alt/></p><p>This model is called an artificial neuron, a perceptron, or simply a unit in neural network literature. We know it as the softmax classifier.</p><p>When a number of these units are connected in layers, we get a multilayer perceptron. When counting layers, we ignore the input layer. So the softmax classifier can be considered a one layer neural network. Here is a neural network picture and the corresponding matrix picture for a two layer model:</p><blockquote><p><img src="../images/neural_net.jpeg" alt="image"/></p></blockquote><blockquote><p><img src="../images/mlp2.jpg" alt="image"/></p></blockquote><p>Here is a neural network picture and the corresponding matrix picture for a three layer model:</p><blockquote><p><img src="../images/neural_net2.jpeg" alt="image"/></p></blockquote><blockquote><p><img src="../images/mlp3.jpg" alt="image"/></p></blockquote><p>We can use the following elementwise notation for the neural network picture (e.g. similar to the one used in <a href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks">UFLDL</a>):</p><p class="math-container">\[x_i^{(l)} = f(b_i^{(l)} + \sum_j w_{ij}^{(l)} x_j^{(l-1)})\]</p><p>Here <span>$x_i^{(l)}$</span> refers to the activation of the <span>$i$</span> th unit in <span>$l$</span> th layer. We are counting the input as the 0&#39;th layer. <span>$f$</span> is the activation function, <span>$b_i^{(l)}$</span> is the bias term. <span>$w_{ij}^{(l)}$</span> is the weight connecting unit <span>$j$</span> from layer <span>$l-1$</span> to unit <span>$i$</span> from layer <span>$l$</span>. The corresponding matrix notation is:</p><p class="math-container">\[x^{(l)} = f(W^{(l)} x^{(l-1)} + b^{(l)})\]</p><h2 id="Programming-Example"><a class="docs-heading-anchor" href="#Programming-Example">Programming Example</a><a id="Programming-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Programming-Example" title="Permalink"></a></h2><p>In this section we introduce several Knet features that make it easier to define complex models. As our working example, we will go through several attempts to define a 3-layer MLP. Here is our first attempt:</p><pre><code class="language-none">function mlp3a(w, x0)
    x1 = relu(w[1] * x0 .+ w[2])
    x2 = relu(w[3] * x1 .+ w[4])
    return w[5] * x2 .+ w[6]
end</code></pre><p>We can identify bad software engineering practices in this definition in that it contains a lot of repetition.</p><p>The key to controlling complexity in computer languages is <strong>abstraction</strong>. Abstraction is the ability to name compound structures built from primitive parts, so they too can be used as primitives.</p><p><strong>Defining new operators</strong></p><p>We could make the definition of mlp3 more compact by defining separate functions for its layers:</p><pre><code class="language-none">function mlp3b(w, x0)
    x1 = relu_layer1(w, x0)
    x2 = relu_layer2(w, x1)
    return pred_layer3(w, x2)
end

function relu_layer1(w, x)
    return relu(w[1] * x .+ w[2])
end

function relu_layer2(w, x)
    return relu(w[3] * x .+ w[4])
end

function pred_layer3(x)
    return w[5] * x .+ w[6]
end</code></pre><p>This may make the definition of <code>mlp3b</code> a bit more readable. But it does not reduce the overall length of the program. The helper functions like <code>relu_layer1</code> and <code>relu_layer2</code> are too similar except for the weights they use and can be reduced to a single function.</p><p><strong>Increasing the number of layers</strong></p><p>We can define a more general mlp model of arbitrary length. With weights of length 2n, the following model will have n layers, n-1 layers having the relu non-linearity:</p><pre><code class="language-none">function mlp_nlayer(w,x)
    for i=1:2:length(w)-2
        x = relu(w[i] * x .+ w[i+1]))
    end
    return w[end-1] * x .+ w[end]
end</code></pre><p>In this example stacking the layers in a loop saved us only two lines, but the difference can be more significant in deeper models.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ul><li>http://neuralnetworksanddeeplearning.com/chap4.html</li><li>http://www.deeplearningbook.org/contents/mlp.html</li><li>http://cs231n.github.io/neural-networks-1</li><li>http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetwork</li><li>http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../softmax/">« Softmax Classification</a><a class="docs-footer-nextpage" href="../cnn/">Convolutional Neural Networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 22 February 2021 13:54">Monday 22 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
