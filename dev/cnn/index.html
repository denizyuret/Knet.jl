<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Convolutional Neural Networks · Knet.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Knet.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../install/">Setting up Knet</a></li><li><a class="tocitem" href="../tutorial/">Introduction to Knet</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul></li><li><span class="tocitem">Textbook</span><ul><li><a class="tocitem" href="../backprop/">Backpropagation and SGD</a></li><li><a class="tocitem" href="../softmax/">Softmax Classification</a></li><li><a class="tocitem" href="../mlp/">Multilayer Perceptrons</a></li><li class="is-active"><a class="tocitem" href>Convolutional Neural Networks</a><ul class="internal"><li><a class="tocitem" href="#Motivation"><span>Motivation</span></a></li><li><a class="tocitem" href="#Convolution"><span>Convolution</span></a></li><li><a class="tocitem" href="#Pooling"><span>Pooling</span></a></li><li><a class="tocitem" href="#Normalization"><span>Normalization</span></a></li><li><a class="tocitem" href="#Architectures"><span>Architectures</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../rnn/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../rl/">Reinforcement Learning</a></li><li><a class="tocitem" href="../opt/">Optimization</a></li><li><a class="tocitem" href="../gen/">Generalization</a></li><li><a class="tocitem" href="../nce/">Noise Contrastive Estimation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Textbook</a></li><li class="is-active"><a href>Convolutional Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Convolutional Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/denizyuret/Knet.jl/blob/master/docs/src/cnn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Convolutional-Neural-Networks"><a class="docs-heading-anchor" href="#Convolutional-Neural-Networks">Convolutional Neural Networks</a><a id="Convolutional-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-Neural-Networks" title="Permalink"></a></h1><h2 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h2><p>Let&#39;s say we are trying to build a model that will detect cats in photographs. The average resolution of images in <a href="http://www.image-net.org/challenges/LSVRC/2014">ILSVRC</a> is 482x415, with three channels (RGB) this makes the typical input size 482x415x3=600,090. Each hidden unit connected to the input in a multilayer perceptron would have 600K parameters, a single hidden layer of size 1000 would have 600 million parameters. Too many parameters cause three types of problems: (1) runtime: large models are computationally costly to train and run. (2) memory: today&#39;s GPUs have limited amount of memory (4G-12G) and large networks fill them up quickly. (3) sample complexity: models with a large number of parameters are difficult to train without overfitting: we need a lot of data, strong regularization, and/or a good initialization to learn with large models.</p><p>One problem with the MLP is that it is fully connected: every hidden unit is connected to every other in adjacent layers. The model does not assume any spatial relationships between pixels, in fact we can permute all the pixels in an image and the performance of the MLP would be the same!  We could instead have an architecture where each hidden unit is connected to a small patch of the image, say 40x40. Each such locally connected hidden unit would have 40x40x3=4800 parameters instead of 600K. For the price (in memory) of one fully connected hidden unit (600K), we could have 125 of these locally connected mini-hidden-units (4800 each) with receptive fields spread around the image.</p><p>The second problem with the MLP is that it does not take advantage of the symmetry in the problem: a cat in the lower right corner of the image may be similar to a cat in the upper left corner. This means the local hidden units looking at these two patches can share the same weights. We can take one 40x40 cat filter and apply it to each 40x40 patch in the image taking up only 4800 parameters.</p><p>A <strong>convolutional neural network</strong> (aka CNN or ConvNet) combines these two ideas and uses operations that are local and that share weights. CNNs commonly use three types of operations: <a href="#Convolution">convolution</a>, <a href="#Pooling">pooling</a>, and <a href="#Normalization">normalization</a> which we describe next.</p><h2 id="Convolution"><a class="docs-heading-anchor" href="#Convolution">Convolution</a><a id="Convolution-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution" title="Permalink"></a></h2><p>Convolution is the main operation that provides sparse connectivity and weight sharing.  For simplicity we start describing <a href="#conv_1d">convolution in 1-D</a> using the <a href="../reference/#Knet.Ops20.conv4"><code>conv4</code></a> primitive from Knet. We next look at three keyword options that provide variations on the convolution operation: <a href="#conv_padding"><code>padding</code></a>, <a href="#conv_stride"><code>stride</code></a>, and <a href="#conv_mode"><code>mode</code></a>.  We then describe how <code>conv4</code> handles <a href="#conv_dims">multiple dimensions</a>, <a href="#conv_filters">filters</a>, and <a href="#conv_instances">instances</a> in parallel.</p><p>The relationship between convolution and <a href="#conv_matmul">matrix multiplication</a> allows the use of efficient algorithms developed for matrix multiplication in convolution implementations.  The fact that convolution and matrix multiplication can be implemented in terms of each other clarifies the distinction between CNNs and MLPs as one of efficiency, not representative power.  We end this section by describing <a href="#conv_backprop">backpropagation</a> for convolution.</p><h3 id="conv_1d"><a class="docs-heading-anchor" href="#conv_1d">Convolution in 1-D</a><a id="conv_1d-1"></a><a class="docs-heading-anchor-permalink" href="#conv_1d" title="Permalink"></a></h3><p>Let <span>$w, x$</span> be two 1-D vectors with <span>$W, X$</span> elements respectively. In our examples, we will assume <span>$x$</span> is the input (consider it a 1-D image) and <span>$w$</span> is a filter (aka kernel) with <span>$W&lt;X$</span>. The 1-D convolution operation <span>$y=w\ast x$</span> results in a vector with <span>$Y=X-W+1$</span> elements defined as:</p><p class="math-container">\[y_k \equiv \sum_{i+j=k+W} x_i w_j\]</p><p>or equivalently</p><p class="math-container">\[y_k \equiv \sum_{i=k}^{k+W-1} x_i w_{k+W-i}\]</p><p>where <span>$i\in[1,X], j\in[1,W], k\in[1,Y]$</span>. We get each entry in y by multiplying pairs of matching entries in x and w and summing the results. Matching entries in x and w are the ones whose indices add up to a constant. This can be visualized as flipping w, sliding it over x, and at each step writing their dot product into a single entry in y. Here is an example in Julia you should be able to calculate by hand:</p><pre><code class="language-julia">julia&gt; using Knet
julia&gt; w = reshape([1.0,2.0,3.0], (3,1,1,1))
3×1×1×1 Array{Float64,4}: [1,2,3]
julia&gt; x = reshape([1.0:7.0...], (7,1,1,1))
7×1×1×1 Array{Float64,4}: [1,2,3,4,5,6,7]
julia&gt; y = conv4(w, x)
5×1×1×1 Array{Float64,4}: [10,16,22,28,34]</code></pre><p><code>conv4</code> is the convolution operation in Knet (based on the <a href="https://developer.nvidia.com/cudnn">CUDNN</a> implementation). For reasons that will become clear it works with 4-D and 5-D arrays, so we reshape our 1-D input vectors by adding extra singleton dimensions at the end.  The convolution of <code>w=[1,2,3]</code> and <code>x=[1,2,3,4,5,6,7]</code> gives <code>y=[10,16,22,28,34]</code>. For example, the third element of y, 22, can be obtained by reversing w to [3,2,1] and taking its dot product starting with the third element of x, [3,4,5].</p><h3 id="conv_padding"><a class="docs-heading-anchor" href="#conv_padding">Padding</a><a id="conv_padding-1"></a><a class="docs-heading-anchor-permalink" href="#conv_padding" title="Permalink"></a></h3><p>In the last example, the input x had 7 dimensions, the output y had 5. In image processing applications we typically want to keep x and y the same size. For this purpose we can provide a <code>padding</code> keyword argument to the <code>conv4</code> operator. If <code>padding=k</code>, x will be assumed padded with <code>k</code> zeros on the left and right before the convolution, e.g. <code>padding=1</code> means treat x as <code>[0 1 2 3 4 5 6 7 0]</code>. The default padding is 0. For inputs in D-dimensions we can specify padding with a D-tuple, e.g. <code>padding=(1,2)</code> for 2D, or a single number, e.g. <code>padding=1</code> which is shorthand for <code>padding=(1,1)</code>. The result will have <span>$Y=X+2P-W+1$</span> elements where <span>$P$</span> is the padding size. Therefore to preserve the size of x when W=3 we should use <code>padding=1</code>.</p><pre><code class="language-julia">julia&gt; y = conv4(w, x; padding=(1,0))
7×1×1×1 Array{Float64,4}: [4,10,16,22,28,34,32]</code></pre><p>For example, to calculate the first entry of y, take the dot product of the inverted w, <code>[3,2,1]</code> with the first three elements of the padded x, <code>[0 1 2]</code>. You can see that in order to preserve the input size, <span>$Y=X$</span>, given a filter size <span>$W$</span>, the padding should be set to <span>$P=(W-1)/2$</span>. This will work if <span>$W$</span> is odd.</p><h3 id="conv_stride"><a class="docs-heading-anchor" href="#conv_stride">Stride</a><a id="conv_stride-1"></a><a class="docs-heading-anchor-permalink" href="#conv_stride" title="Permalink"></a></h3><p>In the preceding examples we shift the inverted w by one position after each dot product. In some cases you may want to skip two or more positions. This will effectively reduce the size of the output.  The amount of skip is set by the <code>stride</code> keyword argument of the <code>conv4</code> operation (the default stride is 1). In the following example we set stride to <span>$W$</span> such that the consecutive filter applications are non-overlapping:</p><pre><code class="language-julia">julia&gt; y = conv4(w, x; padding=(1,0), stride=3)
3×1×1×1 Array{Float64,4}: [4,22,32]</code></pre><p>Note that the output has the first, middle, and last values of the previous example, i.e. every third value is kept and the rest are skipped. In general if <code>stride=S</code> and <code>padding=P</code>, the size of the output will be:</p><p class="math-container">\[Y = 1 + \left\lfloor\frac{X+2P-W}{S}\right\rfloor\]</p><h3 id="conv_mode"><a class="docs-heading-anchor" href="#conv_mode">Mode</a><a id="conv_mode-1"></a><a class="docs-heading-anchor-permalink" href="#conv_mode" title="Permalink"></a></h3><p>The convolution operation we have used so far flips the convolution kernel before multiplying it with the input. To take our first 1-D convolution example with</p><p class="math-container">\[y_1 = x_1 w_W + x_2 w_{W-1} + x_3 w_{W-2} + \ldots \\
y_2 = x_2 w_W + x_3 w_{W-1} + x_4 w_{W-2} + \ldots \\
\ldots\]</p><p>We could also perform a similar operation without kernel flipping:</p><p class="math-container">\[y_1 = x_1 w_1 + x_2 w_2 + x_3 w_3 + \ldots \\
y_2 = x_2 w_1 + x_3 w_2 + x_4 w_3 + \ldots \\
\ldots\]</p><p>This variation is called cross-correlation. The two modes are specified in Knet by choosing one of the following as the value of the <code>mode</code> keyword:</p><ul><li><code>0</code> for convolution</li><li><code>1</code> for cross-correlation</li></ul><p>This option would be important if we were hand designing our filters. However the mode does not matter for CNNs where the filters are learnt from data, the CNN will simply learn an inverted version of the filter if necessary.</p><h3 id="conv_dims"><a class="docs-heading-anchor" href="#conv_dims">More Dimensions</a><a id="conv_dims-1"></a><a class="docs-heading-anchor-permalink" href="#conv_dims" title="Permalink"></a></h3><p>When the input x has multiple dimensions convolution is defined similarly. In particular the filter w has the same number of dimensions but typically smaller size. The convolution operation flips w in each dimension and slides it over x, calculating the sum of elementwise products at every step. The formulas we have given above relating the output size to the input and filter sizes, padding and stride parameters apply independently for each dimension.</p><p>Knet supports 2D and 3D convolutions. The inputs and the filters have two extra dimensions at the end which means we use 4D and 5D arrays for 2D and 3D convolutions. Here is a 2D convolution example:</p><pre><code class="language-julia">julia&gt; w = reshape([1.0:4.0...], (2,2,1,1))
2×2×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 1.0  3.0
 2.0  4.0
julia&gt; x = reshape([1.0:9.0...], (3,3,1,1))
3×3×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 1.0  4.0  7.0
 2.0  5.0  8.0
 3.0  6.0  9.0
julia&gt; y = conv4(w, x)
2×2×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 23.0  53.0
 33.0  63.0</code></pre><p>To see how this result comes about, note that when you flip w in both dimensions you get:</p><pre><code class="language-none">4 2
3 1</code></pre><p>Multiplying this elementwise with the upper left corner of x:</p><pre><code class="language-none">1 4
2 5</code></pre><p>and adding the results gives you the first entry 23.</p><p>The <code>padding</code> and <code>stride</code> options work similarly in multiple dimensions and can be specified as tuples: <code>padding=(1,2)</code> means a padding width of 1 along the first dimension and 2 along the second dimension for a 2D convolution. You can use <code>padding=1</code> as a shorthand for <code>padding=(1,1)</code>.</p><h3 id="conv_filters"><a class="docs-heading-anchor" href="#conv_filters">Multiple filters</a><a id="conv_filters-1"></a><a class="docs-heading-anchor-permalink" href="#conv_filters" title="Permalink"></a></h3><p>So far we have been ignoring the extra dimensions at the end of our convolution arrays. Now we are ready to put them to use. A D-dimensional input image is typically represented as a D+1 dimensional array with dimensions:</p><p class="math-container">\[[ X_1, \ldots, X_D, C_x ]\]</p><p>The first D dimensions <span>$X_1\ldots X_D$</span> determine the spatial extent of the image. The last dimension <span>$C_x$</span> is the number of channels (aka slices, frames, maps, filters). The definition and number of channels is application dependent. We use <span>$C_x=3$</span> for RGB images representing the intensity in three colors: red, green, and blue. For grayscale images we have a single channel, <span>$C_x=1$</span>. If you were developing a model for chess, we could have <span>$C_x=12$</span>, each channel representing the locations of a different piece type.</p><p>In an actual CNN we do not typically hand-code the filters. Instead we tell the network: &quot;here are 1000 randomly initialized filters, you go ahead and turn them into patterns useful for my task.&quot; This means we usually work with banks of multiple filters simultaneously and GPUs have optimized operations for such filter banks. The dimensions of a typical filter bank are:</p><p class="math-container">\[[ W_1, \ldots, W_D, C_x, C_y ]\]</p><p>The first D dimensions <span>$W_1\ldots W_D$</span> determine the spatial extent of the filters. The next dimension <span>$C_x$</span> is the number of input channels, i.e. the number of filters from the previous layer, or the number of color channels of the input image. The last dimension <span>$C_y$</span> is the number of output channels, i.e. the number of filters in this layer.</p><p>If we take an input of size <span>$[X_1,\ldots, X_D,C_x]$</span> and apply a filter bank of size <span>$[W_1,\ldots,W_D,C_x,C_y]$</span> using padding <span>$[P_1,\ldots,P_D]$</span> and stride <span>$[S_1,\ldots,S_D]$</span> the resulting array will have dimensions:</p><p class="math-container">\[[ W_1, \ldots, W_D, C_x, C_y ] \ast [ X_1, \ldots, X_D, C_x ] 
\Rightarrow [ Y_1, \ldots, Y_D, C_y ] \\
\mathrm{where } Y_i = 1 + \left\lfloor\frac{X_i+2P_i-W_i}{S_i}\right\rfloor\]</p><p>As an example let&#39;s start with an input image of 256x256 pixels and 3 RGB channels. We&#39;ll first apply 25 filters of size 5x5 and <code>padding=2</code>, then 50 filters of size 3x3 and <code>padding=1</code>, and finally 75 filters of size 3x3 and <code>padding=1</code>. Here are the dimensions we will get:</p><p class="math-container">\[[ 256, 256, 3 ] \ast [ 5, 5, 3, 25 ] \Rightarrow [ 256, 256, 25 ] \\
[ 256, 256, 25] \ast [ 3, 3, 25,50 ] \Rightarrow [ 256, 256, 50 ] \\
[ 256, 256, 50] \ast [ 3, 3, 50,75 ] \Rightarrow [ 256, 256, 75 ]\]</p><p>Note that the number of input channels of the input data and the filter bank always match. In other words, a filter covers only a small part of the spatial extent of the input but all of its channel depth.</p><h3 id="conv_instances"><a class="docs-heading-anchor" href="#conv_instances">Multiple instances</a><a id="conv_instances-1"></a><a class="docs-heading-anchor-permalink" href="#conv_instances" title="Permalink"></a></h3><p>In addition to processing multiple filters in parallel, we implement CNNs with minibatching, i.e. process multiple inputs in parallel to fully utilize GPUs. A minibatch of D-dimensional images is represented as a D+2 dimensional array:</p><p class="math-container">\[[ X_1, \ldots, X_D, C_x, N ]\]</p><p>where <span>$C_x$</span> is the number of channels as before, and N is the number of images in a minibatch. The convolution implementation in Knet/CUDNN use <span>$D+2$</span> dimensional arrays for both images and filters. We used 1 for the extra dimensions in our first examples, in effect using a single channel and a single image minibatch.</p><p>If we apply a filter bank of size <span>$[W_1, \ldots, W_D, C_x, C_y]$</span> to the minibatch given above the output size would be:</p><p class="math-container">\[[ W_1, \ldots, W_D, C_x, C_y ] \ast [ X_1, \ldots, X_D, C_x, N ] 
\Rightarrow [ Y_1, \ldots, Y_D, C_y, N ] \\
\mathrm{where } Y_i = 1 + \left\lfloor\frac{X_i+2P_i-W_i}{S_i}\right\rfloor\]</p><p>If we used a minibatch size of 128 in the previous example with 256x256 images, the sizes would be:</p><p class="math-container">\[[ 256, 256, 3, 128 ] \ast [ 5, 5, 3, 25 ] \Rightarrow [ 256, 256, 25, 128 ] \\
[ 256, 256, 25, 128] \ast [ 3, 3, 25,50 ] \Rightarrow [ 256, 256, 50, 128 ] \\
[ 256, 256, 50, 128] \ast [ 3, 3, 50,75 ] \Rightarrow [ 256, 256, 75, 128 ]\]</p><p>basically adding an extra dimension of 128 at the end of each data array.</p><p>By the way, the arrays in this particular example already exceed 5GB of storage, so you would want to use a smaller minibatch size if you had a K20 GPU with 4GB of RAM.</p><p>Note: All the dimensions given above are for column-major languages like Julia. CUDNN uses row-major notation, so all the dimensions would be reversed, e.g. <span>$[N,C_x,X_D,\ldots,X_1]$</span>.</p><h3 id="conv_matmul"><a class="docs-heading-anchor" href="#conv_matmul">Convolution vs matrix multiplication</a><a id="conv_matmul-1"></a><a class="docs-heading-anchor-permalink" href="#conv_matmul" title="Permalink"></a></h3><p>Convolution can be turned into a matrix multiplication, where certain entries in the matrix are constrained to be the same. The motivation is to be able to use efficient algorithms for matrix multiplication in order to perform convolution. The drawback is the large amount of memory needed due to repeated entries or sparse representations.</p><p>Here is a matrix implementation for our first convolution example <span>$w=[1\ldots 3],\,\,x=[1\ldots 7],\,\,w\ast x = [10,16,22,28,34]$</span>:</p><blockquote><p><img src="../images/im2col1a.jpg" alt="image"/></p></blockquote><p>In this example we repeated the entries of the filter on multiple rows of a sparse matrix with shifted positions. Alternatively we can repeat the entries of the input to place each local patch on a separate column of an input matrix:</p><blockquote><p><img src="../images/im2col1b.jpg" alt="image"/></p></blockquote><p>The first approach turns w into a <span>$Y\times X$</span> sparse matrix, wheras the second turns x into a <span>$W\times Y$</span> dense matrix.</p><p>For 2-D images, typically the second approach is used: the local patches of the image used by convolution are stretched out to columns of an input matrix, an operation commonly called <code>im2col</code>. Each convolutional filter is stretched out to rows of a filter matrix. After the matrix multiplication the resulting array is reshaped into the proper output dimensions. The following figure illustrates these operations on a small example:</p><blockquote><p><img src="../images/im2col2.jpg" alt="image"/></p></blockquote><p>It is also possible to go in the other direction, i.e. implement matrix multiplication (i.e. a fully connected layer) in terms of convolution. This conversion is useful when we want to build a network that can be applied to inputs of different sizes: the matrix multiplication would fail, but the convolution will give us outputs of matching sizes. Consider a fully connected layer with a weight matrix W of size <span>$K\times D$</span> mapping a D-dimensional input vector x to a K-dimensional output vector y. We can consider each of the K rows of the W matrix a convolution filter. The following example shows how we can reshape the arrays and use convolution for matrix multiplication:</p><pre><code class="language-julia">julia&gt; using Knet
julia&gt; x = reshape([1.0:3.0...], (3,1))
3×1 Array{Float64,2}:
 1.0
 2.0
 3.0
julia&gt; w = reshape([1.0:6.0...], (2,3))
2×3 Array{Float64,2}:
 1.0  3.0  5.0
 2.0  4.0  6.0
julia&gt; y = w * x
2×1 Array{Float64,2}:
 22.0
 28.0
julia&gt; x2 = reshape(x, (3,1,1,1))
3×1×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 1.0
 2.0
 3.0
julia&gt; w2 = reshape(Array(w)&#39;, (3,1,1,2))
3×1×1×2 Array{Float64,4}:
[:, :, 1, 1] =
 1.0
 3.0
 5.0
[:, :, 1, 2] =
 2.0
 4.0
 6.0
julia&gt; y2 = conv4(w2, x2; mode=1)
1×1×2×1 Array{Float64,4}:
[:, :, 1, 1] =
 22.0
[:, :, 2, 1] =
 28.0</code></pre><p>In addition to computational concerns, these examples also show that a fully connected layer can emulate a convolutional layer given the right weights and vice versa, i.e. convolution does not get us any extra representational power. However it does get us representational and statistical efficiency, i.e. the functions we would like to approximate are often expressed with significantly fewer parameters using convolutional layers and thus require fewer examples to train.</p><h3 id="conv_backprop"><a class="docs-heading-anchor" href="#conv_backprop">Backpropagation</a><a id="conv_backprop-1"></a><a class="docs-heading-anchor-permalink" href="#conv_backprop" title="Permalink"></a></h3><p>Convolution is a linear operation consisting of additions and multiplications, so its backward pass is not very complicated except for the indexing. Just like the backward pass for matrix multiplication can be expressed as another matrix multiplication, the backward pass for convolution (at least if we use stride=1) can be expressed as another convolution. We will derive the backward pass for a 1-D example using the cross-correlation mode (no kernel flipping) to keep things simple. We will denote the cross-correlation operation with <span>$\star$</span> to distinguish it from convolution denoted with <span>$\ast$</span>. Here are the individual entries of <span>$y=w\star x$</span>:</p><p class="math-container">\[y_1 = x_1 w_1 + x_2 w_2 + x_3 w_3 + \ldots \\
y_2 = x_2 w_1 + x_3 w_2 + x_4 w_3 + \ldots \\
y_3 = x_3 w_1 + x_4 w_2 + x_5 w_3 + \ldots \\
\ldots\]</p><p>As you can see, because of weight sharing the same w entry is used in computing multiple y entries. This means a single w entry effects the objective function through multiple paths and these effects need to be added. Denoting <span>$\partial J/\partial y_i$</span> as <span>$y_i&#39;$</span> for brevity we have:</p><p class="math-container">\[w_1&#39; = x_1 y_1&#39; + x_2 y_2&#39; + \ldots \\
w_2&#39; = x_2 y_1&#39; + x_3 y_2&#39; + \ldots \\
w_3&#39; = x_3 y_1&#39; + x_4 y_2&#39; + \ldots \\
\ldots \\\]</p><p>which can be recognized as another cross-correlation operation, this time between <span>$x$</span> and <span>$y&#39;$</span>. This allows us to write <span>$w&#39;=y&#39;\star x$</span>.</p><p>Alternatively, we can use the equivalent matrix multiplication operation from the last section to derive the backward pass:</p><blockquote><p><img src="../images/xcor-im2col-forw.jpg" alt="image"/></p></blockquote><p>If <span>$r$</span> is the matrix with repeated <span>$x$</span> entries in this picture, we have <span>$y=wr$</span>. Remember that the backward pass for matrix multiplication <span>$y=wr$</span> is <span>$w&#39;=y&#39;r^T$</span>:</p><blockquote><p><img src="../images/xcor-im2col-back.jpg" alt="image"/></p></blockquote><p>which can be recognized as the matrix multiplication equivalent of the cross correlation operation <span>$w&#39;=y&#39;\star x$</span>.</p><p>Here is the gradient for the input:</p><p class="math-container">\[\begin{aligned}
&amp; x_1&#39; = w_1 y_1&#39; \\
&amp; x_2&#39; = w_2 y_1&#39; + w_1 y_2&#39; \\
&amp; x_3&#39; = w_3 y_1&#39; + w_2 y_2&#39; + w_1 y_3&#39; \\
&amp; \ldots
\end{aligned}\]</p><p>You can recognize this as a regular convolution between <span>$w$</span> and <span>$y&#39;$</span> with some zero padding.</p><p>The following resources provide more detailed derivations of the backward pass for convolution:</p><ul><li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/convolution.pdf">Goodfellow, I.   (2010)</a>.   Technical report: Multidimensional, downsampled convolution for   autoencoders. Technical report, Université de Montréal. 312.</li><li><a href="http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf">Bouvrie, J.   (2006)</a>.   Notes on convolutional neural networks.</li><li>UFLDL   <a href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork">tutorial</a>   and   <a href="http://ufldl.stanford.edu/tutorial/supervised/ExerciseConvolutionalNeuralNetwork">exercise</a>   on CNNs.</li></ul><h2 id="Pooling"><a class="docs-heading-anchor" href="#Pooling">Pooling</a><a id="Pooling-1"></a><a class="docs-heading-anchor-permalink" href="#Pooling" title="Permalink"></a></h2><p>It is common practice to use pooling (aka subsampling) layers in between convolution operations in CNNs. Pooling looks at small windows of the input, and computes a single summary statistic, e.g. maximum or average, for each window. A pooling layer basically says: tell me whether this feature exists in a certain region of the image, I don&#39;t care exactly where. This makes the output of the layer invariant to small translations of the input. Pooling layers use large strides, typically as large as the window size, which reduces the size of their output.</p><p>Like convolution, pooling slides a small window of a given size over the input optionally padded with zeros skipping stride pixels every step. In Knet by default there is no padding, the window size is 2, stride is equal to the window size and the pooling operation is max. These default settings reduce each dimension of the input to half the size.</p><h3 id="pool_1d"><a class="docs-heading-anchor" href="#pool_1d">Pooling in 1-D</a><a id="pool_1d-1"></a><a class="docs-heading-anchor-permalink" href="#pool_1d" title="Permalink"></a></h3><p>Here is a 1-D example:</p><pre><code class="language-julia">julia&gt; x = reshape([1.0:6.0...], (6,1,1,1))
6×1×1×1 Array{Float64,4}: [1,2,3,4,5,6]
julia&gt; pool(x)
3×1×1×1 Array{Float64,4}: [2,4,6]</code></pre><p>With window size and stride equal to 2, pooling considers the input windows <span>$[1,2], [3,4], [5,6]$</span> and picks the maximum in each window.</p><h3 id="pool_window"><a class="docs-heading-anchor" href="#pool_window">Window</a><a id="pool_window-1"></a><a class="docs-heading-anchor-permalink" href="#pool_window" title="Permalink"></a></h3><p>The default and most commonly used window size is 2, however other window sizes can be specified using the <code>window</code> keyword. For D-dimensional inputs the size can be specified using a D-tuple, e.g. <code>window=(2,3)</code> for 2-D, or a single number, e.g. <code>window=3</code> which is shorthand for <code>window=(3,3)</code> in 2-D. Here is an example using a window size of 3 instead of the default 2:</p><pre><code class="language-julia">julia&gt; x = reshape([1.0:6.0...], (6,1,1,1))
6×1×1×1 Array{Float64,4}: [1,2,3,4,5,6]
julia&gt; pool(x; window=3)
2×1×1×1 Array{Float64,4}: [3, 6]</code></pre><p>With a window and stride of 3 (the stride is equal to window size by default), pooling considers the input windows <span>$[1,2,3],[4,5,6]$</span>, and writes the maximum of each window to the output. If the input size is <span>$X$</span>, and stride is equal to the window size <span>$W$</span>, the output will have <span>$Y=\lfloor X/W\rfloor$</span> elements.</p><h3 id="pool_padding"><a class="docs-heading-anchor" href="#pool_padding">Padding</a><a id="pool_padding-1"></a><a class="docs-heading-anchor-permalink" href="#pool_padding" title="Permalink"></a></h3><p>The amount of zero padding is specified using the <code>padding</code> keyword argument just like convolution. Padding is 0 by default. For D-dimensional inputs padding can be specified as a tuple such as <code>padding=(1,2)</code>, or a single number <code>padding=1</code> which is shorthand for <code>padding=(1,1)</code> in 2-D. Here is a 1-D example:</p><pre><code class="language-julia">julia&gt; x = reshape([1.0:6.0...], (6,1,1,1))
6×1×1×1 Array{Float64,4}: [1,2,3,4,5,6]

julia&gt; pool(x; padding=(1,0))
4×1×1×1 Array{Float64,4}: [1,3,5,6]</code></pre><p>In this example, window=stride=2 by default and the padding size is 1, so the input is treated as <span>$[0,1,2,3,4,5,6,0]$</span> and split into windows of <span>$[0,1],[2,3],[4,5],[6,0]$</span> and the maximum of each window is written to the output.</p><p>With padding size <span>$P$</span>, if the input size is <span>$X$</span>, and stride is equal to the window size <span>$W$</span>, the output will have <span>$Y=\lfloor (X+2P)/W\rfloor$</span> elements.</p><h3 id="pool_stride"><a class="docs-heading-anchor" href="#pool_stride">Stride</a><a id="pool_stride-1"></a><a class="docs-heading-anchor-permalink" href="#pool_stride" title="Permalink"></a></h3><p>The pooling stride is equal to the window size by default (as opposed to the convolution case, where it is 1 by default). This is most common in practice but other strides can be specified using tuples e.g. <code>stride=(1,2)</code> or numbers e.g. <code>stride=1</code>. Here is a 1-D example with a stride of 4 instead of the default 2:</p><pre><code class="language-julia">julia&gt; x = reshape([1.0:10.0...], (10,1,1,1))
10×1×1×1 Array{Float64,4}: [1,2,3,4,5,6,7,8,9,10]

julia&gt; pool(x; stride=4)
4×1×1×1 Array{Float64,4}: [2, 6, 10]</code></pre><p>In general, when we have an input of size <span>$X$</span> and pool with window size <span>$W$</span>, padding <span>$P$</span>, and stride <span>$S$</span>, the size of the output will be:</p><p class="math-container">\[Y = 1 + \left\lfloor\frac{X+2P-W}{S}\right\rfloor\]</p><h3 id="pool_mode"><a class="docs-heading-anchor" href="#pool_mode">Mode</a><a id="pool_mode-1"></a><a class="docs-heading-anchor-permalink" href="#pool_mode" title="Permalink"></a></h3><p>There are three pooling operations defined by CUDNN used for summarizing each window:</p><ul><li><code>CUDNN_POOLING_MAX</code></li><li><code>CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</code></li><li><code>CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING</code></li></ul><p>These options can be specified as the value of the <code>mode</code> keyword argument to the <code>pool</code> operation. The default is <code>0</code> (max pooling) which we have been using so far. The last two compute averages, and differ in whether to include or exclude the padding zeros in these averages. <code>mode</code> should be <code>1</code> for averaging including padding, and <code>2</code> for averaging excluding padding. For example, with input <span>$x=[1,2,3,4,5,6]$</span>, <code>window=stride=2</code>, and <code>padding=1</code> we have the following outputs with the three options:</p><pre><code class="language-none">mode=0 =&gt; [1,3,5,6]
mode=1 =&gt; [0.5, 2.5, 4.5, 3.0]
mode=2 =&gt; [1.0, 2.5, 4.5, 6.0]</code></pre><h3 id="pool_dims"><a class="docs-heading-anchor" href="#pool_dims">More Dimensions</a><a id="pool_dims-1"></a><a class="docs-heading-anchor-permalink" href="#pool_dims" title="Permalink"></a></h3><p>D-dimensional inputs are pooled with D-dimensional windows, the size of each output dimension given by the 1-D formulas above. Here is a 2-D example with default options, i.e. window=stride=(2,2), padding=(0,0), mode=max:</p><pre><code class="language-julia">julia&gt; x = reshape([1.0:16.0...], (4,4,1,1))
4×4×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 1.0  5.0   9.0  13.0
 2.0  6.0  10.0  14.0
 3.0  7.0  11.0  15.0
 4.0  8.0  12.0  16.0

julia&gt; pool(x)
2×2×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 6.0  14.0
 8.0  16.0</code></pre><h3 id="pool_instances"><a class="docs-heading-anchor" href="#pool_instances">Multiple channels and instances</a><a id="pool_instances-1"></a><a class="docs-heading-anchor-permalink" href="#pool_instances" title="Permalink"></a></h3><p>As we saw in convolution, each data array has two extra dimensions in addition to the spatial dimensions: <span>$[ X_1, \ldots, X_D, C_x, N ]$</span> where <span>$C_x$</span> is the number of channels and <span>$N$</span> is the number of instances in a minibatch.</p><p>When the number of channels is greater than 1, the pooling operation is performed independently on each channel, e.g. for each patch, the maximum/average in each channel is computed independently and copied to the output. Here is an example with two channels:</p><pre><code class="language-julia">julia&gt; x = rand(4,4,2,1)
4×4×2×1 Array{Float64,4}:
[:, :, 1, 1] =
 0.880221  0.738729  0.317231   0.990521
 0.626842  0.562692  0.339969   0.92469
 0.416676  0.403625  0.352799   0.46624
 0.566254  0.634703  0.0632812  0.0857779

[:, :, 2, 1] =
 0.300799  0.407623   0.26275   0.767884
 0.217025  0.0055375  0.623168  0.957374
 0.154975  0.246693   0.769524  0.628197
 0.259161  0.648074   0.333324  0.46305

julia&gt; pool(x)
2×2×2×1 Array{Float64,4}:
[:, :, 1, 1] =
 0.880221  0.990521
 0.634703  0.46624

[:, :, 2, 1] =
 0.407623  0.957374
 0.648074  0.769524</code></pre><p>When the number of instances is greater than 1, i.e. we are using minibatches, the pooling operation similarly runs in parallel on all the instances:</p><pre><code class="language-julia">julia&gt; x = rand(4,4,1,2)
4×4×1×2 Array{Float64,4}:
[:, :, 1, 1] =
 0.155228  0.848345  0.629651  0.262436
 0.729994  0.320431  0.466628  0.0293943
 0.374592  0.662795  0.819015  0.974298
 0.421283  0.83866   0.385306  0.36081

[:, :, 1, 2] =
 0.0562608  0.598084  0.0231604  0.232413
 0.71073    0.411324  0.28688    0.287947
 0.997445   0.618981  0.471971   0.684064
 0.902232   0.570232  0.190876   0.339076

julia&gt; pool(x)
2×2×1×2 Array{Float64,4}:
[:, :, 1, 1] =
 0.848345  0.629651
 0.83866   0.974298

[:, :, 1, 2] =
 0.71073   0.287947
 0.997445  0.684064</code></pre><h2 id="Normalization"><a class="docs-heading-anchor" href="#Normalization">Normalization</a><a id="Normalization-1"></a><a class="docs-heading-anchor-permalink" href="#Normalization" title="Permalink"></a></h2><p>Draft...</p><p>Karpathy says: &quot;Many types of normalization layers have been proposed for use in ConvNet architectures, sometimes with the intentions of implementing inhibition schemes observed in the biological brain. However, these layers have recently fallen out of favor because in practice their contribution has been shown to be minimal, if any.&quot; (<a href="http://cs231n.github.io/convolutional-networks/#norm">http://cs231n.github.io/convolutional-networks/#norm</a>) Batch normalization may be an exception, as it is used in modern architectures.</p><p>Here are some references for normalization operations:</p><p>Implementations:</p><ul><li>Alex Krizhevsky&#39;s cuda-convnet library API.   (<a href="https://code.google.com/archive/p/cuda-convnet/wikis/LayerParams.wiki#Local_response_normalization_layer_(same_map)">https://code.google.com/archive/p/cuda-convnet/wikis/LayerParams.wiki#Local<em>response</em>normalization<em>layer</em>(same_map)</a>)</li><li><a href="http://caffe.berkeleyvision.org/tutorial/layers.html">http://caffe.berkeleyvision.org/tutorial/layers.html</a></li><li><a href="http://lasagne.readthedocs.org/en/latest/modules/layers/normalization.html">http://lasagne.readthedocs.org/en/latest/modules/layers/normalization.html</a></li></ul><p>Divisive normalisation (DivN):</p><ul><li>S. Lyu and E. Simoncelli. Nonlinear image representation using   divisive normalization. In CVPR, pages 1–8, 2008.</li></ul><p>Local contrast normalization (LCN):</p><ul><li>N. Pinto, D. D. Cox, and J. J. DiCarlo. Why is real-world visual   object recognition hard? PLoS Computational Biology, 4(1), 2008.</li><li>Jarrett, Kevin, et al. &quot;What is the best multi-stage architecture   for object recognition?.&quot; Computer Vision, 2009 IEEE 12th   International Conference on. IEEE, 2009.   (<a href="http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf">http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf</a>)</li></ul><p>Local response normalization (LRN):</p><ul><li>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &quot;Imagenet   classification with deep convolutional neural networks.&quot; Advances in   neural information processing systems. 2012.   (<a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2012_0534.pdf">http://machinelearning.wustl.edu/mlpapers/paper<em>files/NIPS2012</em>0534.pdf</a>)</li></ul><p>Batch Normalization: This is more of an optimization topic.</p><ul><li>Ioffe, Sergey, and Christian Szegedy. &quot;Batch normalization:   Accelerating deep network training by reducing internal covariate   shift.&quot; arXiv preprint arXiv:1502.03167 (2015).   (<a href="http://arxiv.org/abs/1502.03167/">http://arxiv.org/abs/1502.03167/</a>)</li></ul><h2 id="Architectures"><a class="docs-heading-anchor" href="#Architectures">Architectures</a><a id="Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures" title="Permalink"></a></h2><p>We have seen a number of new operations: convolution, pooling, filters etc. How to best put these together to form a CNN is still an active area of research. In this section we summarize common patterns of usage in recent work based on <a href="http://cs231n.github.io/convolutional-networks">(Karpathy, 2016)</a>.</p><ul><li>The operations in convolutional networks are usually ordered into   several layers of convolution-bias-activation-pooling sequences.   Note that the convolution-bias-activation sequence is an efficient   way to implement the common neural net function <span>$f(wx+b)$</span> for a   locally connected and weight sharing hidden layer.</li><li>The convolutional layers are typically followed by a number of fully   connected layers that end with a softmax layer for prediction (if we   are training for a classification problem).</li><li>It is preferrable to have multiple convolution layers with small   filter sizes rather than a single layer with a large filter size.   Consider three convolutional layers with a filter size of   3x3. The units in the top layer have receptive fields of   size 7x7. Compare this with a single layer with a filter   size of 7x7. The three layer architecture has two   advantages: The units in the single layer network is restricted to   linear decision boundaries, whereas the three layer network can be   more expressive. Second, if we assume C channels, the parameter   tensor for the single layer network has size <span>$[7,7,C,C]$</span> whereas the   three layer network has three tensors of size <span>$[3,3,C,C]$</span> i.e. a   smaller number of parameters. The one disadvantage of the three   layer network is the extra storage required to store the   intermediate results for backpropagation.</li><li>Thus common settings for convolution use 3x3 filters with   <code>stride = padding = 1</code> (which incidentally preserves the input   size). The one exception may be a larger filter size used in the   first layer which is applied to the image pixels. This will save   memory when the input is at its largest, and linear functions may be   sufficient to express the low level features at this stage.</li><li>The pooling operation may not be present in every layer. Keep in   mind that pooling destroys information and having several   convolutional layers without pooling may allow more complex features   to be learnt. When pooling is present it is best to keep the window   size small to minimize information loss. The common settings for   pooling are <code>window = stride = 2, padding = 0</code>, which halves the   input size in each dimension.</li></ul><p>Beyond these general guidelines, you should look at the architectures used by successful models in the literature. Some examples are LeNet <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">(LeCun et al. 1998)</a>, AlexNet <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">(Krizhevsky et al. 2012)</a>, ZFNet <a href="http://arxiv.org/abs/1311.2901">(Zeiler and Fergus, 2013)</a>, GoogLeNet <a href="http://arxiv.org/abs/1409.4842">(Szegedy et al. 2014)</a>, VGGNet <a href="http://arxiv.org/abs/1409.1556">(Simonyan and Zisserman, 2014)</a>, and ResNet <a href="http://arxiv.org/abs/1512.03385">(He et al. 2015)</a>.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ul><li>Design a filter that shifts a given image one pixel to right.</li><li>Design an image filter that has 0 output in regions of uniform   color, but nonzero output at edges where the color changes.</li><li>If your input consisted of two consecutive frames of video, how   would you detect motion using convolution?</li><li>Can you implement matrix-vector multiplication in terms of   convolution? How about matrix-matrix multiplication? Do you need   reshape operations?</li><li>Can you implement convolution in terms of matrix multiplication?</li><li>Can you implement elementwise broadcasting multiplication in terms   of convolution?</li></ul><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ul><li>Some of this chapter was based on the excellent lecture notes from:   <a href="http://cs231n.github.io/convolutional-networks">http://cs231n.github.io/convolutional-networks</a></li><li>Christopher Olah&#39;s blog has very good visual explanations (thanks to   Melike Softa for the reference):   <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular">http://colah.github.io/posts/2014-07-Conv-Nets-Modular</a></li><li><a href="Understanding Neural Networks Through Deep Visualization">http://yosinski.com/deepvis</a></li><li><a href="Feature Visualization">https://distill.pub/2017/feature-visualization/</a></li><li><a href="The Building Blocks of Interpretability">https://distill.pub/2018/building-blocks/</a></li><li><a href="http://ufldl.stanford.edu">UFLDL</a> (or its <a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial">old   version</a>)   is an online tutorial with programming examples and explicit   gradient derivations covering   <a href="http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution">convolution</a>,   <a href="http://ufldl.stanford.edu/tutorial/supervised/Pooling">pooling</a>,   and   <a href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork">CNNs</a>.</li><li>Hinton&#39;s video lecture and presentation at Coursera (Lec 5):   <a href="https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec5.pdf">https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec5.pdf</a></li><li>For a derivation of gradients see:   <a href="http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf">http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf</a> or   <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/convolution.pdf">http://www.iro.umontreal.ca/~lisa/pointeurs/convolution.pdf</a></li><li>The CUDNN manual has more details about the convolution API:   <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></li><li><a href="http://deeplearning.net/tutorial/lenet.html">http://deeplearning.net/tutorial/lenet.html</a></li><li><a href="http://www.denizyuret.com/2014/04/on-emergence-of-visual-cortex-receptive.html">http://www.denizyuret.com/2014/04/on-emergence-of-visual-cortex-receptive.html</a></li><li><a href="http://neuralnetworksanddeeplearning.com/chap6.html">http://neuralnetworksanddeeplearning.com/chap6.html</a></li><li><a href="http://www.deeplearningbook.org/contents/convnets.html">http://www.deeplearningbook.org/contents/convnets.html</a></li><li><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp</a></li><li><a href="http://scs.ryerson.ca/~aharley/vis/conv/">http://scs.ryerson.ca/~aharley/vis/conv/</a> has a nice visualization   of an MNIST CNN. (Thanks to Fatih Ozhamaratli for the reference).</li><li><a href="http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet">http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet</a>   visualizing popular CNN architectures side by side with mxnet.</li><li><a href="http://cs231n.github.io/understanding-cnn">http://cs231n.github.io/understanding-cnn</a> visualizing what   convnets learn.</li><li><a href="https://arxiv.org/abs/1603.07285">https://arxiv.org/abs/1603.07285</a> A guide to convolution arithmetic   for deep learning</li><li>Reading (architectures): <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf">cs231n Architecture Slides</a></li><li>Reading (visualization): <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf">cs231n Visualization Slides</a>, <a href="http://cs231n.github.io/understanding-cnn">cs231n Visualization Notes</a>, <a href="https://distill.pub/2018/building-blocks/">Distillpub visualization article</a>, <a href="http://yosinski.com/deepvis">Yosinski blog</a>, <a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM">video</a>, <a href="http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf">paper</a>, <a href="https://github.com/yosinski/deep-visualization-toolbox">repo</a></li><li><a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">A Simple Guide to the Versions of the Inception Network</a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mlp/">« Multilayer Perceptrons</a><a class="docs-footer-nextpage" href="../rnn/">Recurrent Neural Networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 31 January 2021 20:23">Sunday 31 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
