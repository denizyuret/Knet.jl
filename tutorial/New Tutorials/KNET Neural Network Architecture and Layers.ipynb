{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knet Neural Networks\n",
    "***\n",
    "In this an the following notebook, we will analyze the procedure of defining, training and evaluating neural networks. \n",
    "* Objective: Learning construction a model like the LeNet given in [Quick Start](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/15.quickstart.ipynb) with a thorough explanation of each part\n",
    "* Prerequisites: [Julia arrays](https://docs.julialang.org/en/v1/manual/arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Knet, there are two ways to create a neural network, one being using the built-in Knet structs and the other being defining hand-written callable objects that accepts an array (matrix or vector) and outputs the result of the wanted logic in the right dimension. We will begin with custom layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Knet if not already imported or using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îå Warning: This version of CUDA.jl only supports NVIDIA drivers for CUDA 10.1 or higher (yours is for CUDA 8.0.0)\n",
      "‚îî @ CUDA C:\\Users\\PC\\.julia\\packages\\CUDA\\gKMm0\\src\\initialization.jl:111\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InitError: Could not find a suitable CUDA installation\nduring initialization of module Knet",
     "output_type": "error",
     "traceback": [
      "InitError: Could not find a suitable CUDA installation\nduring initialization of module Knet",
      "",
      "Stacktrace:",
      " [1] error(::String) at .\\error.jl:33",
      " [2] __runtime_init__() at C:\\Users\\PC\\.julia\\packages\\CUDA\\gKMm0\\src\\initialization.jl:114",
      " [3] macro expansion at C:\\Users\\PC\\.julia\\packages\\CUDA\\gKMm0\\src\\initialization.jl:32 [inlined]",
      " [4] macro expansion at .\\lock.jl:183 [inlined]",
      " [5] _functional(::Bool) at C:\\Users\\PC\\.julia\\packages\\CUDA\\gKMm0\\src\\initialization.jl:26",
      " [6] functional(::Bool) at C:\\Users\\PC\\.julia\\packages\\CUDA\\gKMm0\\src\\initialization.jl:19",
      " [7] functional at C:\\Users\\PC\\.julia\\packages\\CUDA\\gKMm0\\src\\initialization.jl:18 [inlined]",
      " [8] __init__() at C:\\Users\\PC\\.julia\\packages\\Knet\\OYNCT\\src\\Knet.jl:26",
      " [9] _include_from_serialized(::String, ::Array{Any,1}) at .\\loading.jl:697",
      " [10] _require_search_from_serialized(::Base.PkgId, ::String) at .\\loading.jl:782",
      " [11] _require(::Base.PkgId) at .\\loading.jl:1007",
      " [12] require(::Base.PkgId) at .\\loading.jl:928",
      " [13] require(::Module, ::Symbol) at .\\loading.jl:923",
      " [14] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091",
      " [15] execute_code(::String, ::String) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:27",
      " [16] execute_request(::ZMQ.Socket, ::IJulia.Msg) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:86",
      " [17] #invokelatest#1 at .\\essentials.jl:710 [inlined]",
      " [18] invokelatest at .\\essentials.jl:709 [inlined]",
      " [19] eventloop(::ZMQ.Socket) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\eventloop.jl:8",
      " [20] (::IJulia.var\"#15#18\")() at .\\task.jl:356"
     ]
    }
   ],
   "source": [
    "#using Pkg\n",
    "#Pkg.add(\"Knet\")\n",
    "using Knet\n",
    "#You may see an error if your device does not support CUDA or your CUDA driver is not CUDA 10.1 or higher but you will be\n",
    "#able to use all the functionalities, except GPU operations, in spite of this error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our First Custom Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct dense; w; b; f; end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a sample dense layer includes w (weights), b (bias), f(activation function). Bias and activation functions are not necessary but the field \"w\" is needed in all custom layers to be manipulated during training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d::dense)(x) = d.f.(d.w * mat(x) .+ d.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a functor (a callable object/ function-like object) that multiplies the input with its weights, adds bias, applies the given activation function and returns the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense(i::Int,o::Int,f=relu) = dense(param(o,i), param0(o), f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare a constructor that utilizes built-in \"param\" and \"param0\" functions that return KnetArrays, powerful built-in array that have all the operations a Julia array has but also designed with a focus on GPU opearations, in the appropriate sizes. Usage of Knet arrays are not mandatory however highly encouraged due to performance improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the constructor has a keyword-parameter named \"f\", whose default value is relu. Relu is one of the many built-in activation functions Knet has. Here is a list of all the built-in Knet functions: elu, relu, selu, sigmoid, gelu.\n",
    "For further reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{relu} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `relu` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36mrelu\u001b[39m does not exist."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{elu} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `elu` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36melu\u001b[39m does not exist."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how custom layers are defined in Knet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example custom layer:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv; w; b; f; end\n",
    "(c::Conv)(x) = c.f.(pool(conv4(c.w, x) .+ c.b))\n",
    "Conv(w1,w2,cx,cy,f=relu) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, a struct with the fields including  the \"w\" field representing the weights has been declared. Then, a function-like object that takes an input (matrix, number or vector) and outputs the result of the appliance of the inner logic. This struct makes use of the param and param0 as well. Since this is a convolutional layer, the calculation has different steps. For further reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{conv4} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `conv4` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36mconv4\u001b[39m does not exist."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " @doc conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{pool} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `pool` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36mpool\u001b[39m does not exist."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using built-in Layers\n",
    "**A NOTE FOR THE DEVELOPER TEAMTo include this part, the [layers script](https://github.com/denizyuret/Knet.jl/blob/master/src/layers21/Layers21.jl) must export the layers** <br>\n",
    "Knet offers the following built-in layers with a remarkable option of customization:\n",
    "* [Dense](https://github.com/denizyuret/Knet.jl/blob/master/src/layers21/dense.jl)\n",
    "* [Embed](https://github.com/denizyuret/Knet.jl/blob/master/src/layers21/embed.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer \n",
    "Built-in Dense Layer has the following constructors:\n",
    "\n",
    "* function Dense(weights, bias=nothing; f=nothing, dims=1, dropout=0)\n",
    "* function Dense(wsize::Integer...; f=nothing, dims=1, dropout=0, atype=atype(), binit=zeros,  init=ùëº(‚àö(6/(densein(wsize,dims)+denseout(wsize,dims)))))\n",
    "\n",
    "Although how confusion the definitions may seem at the first glance, they are fairly easy to use. \n",
    "\n",
    "Keyword arguments:\n",
    "* `f=nothing`: apply activation function to output if not nothing\n",
    "* `dims=1`: number of input dimensions in the weight tensor\n",
    "* `dropout=0`: apply dropout with this probability to input if non-zero\n",
    "* `atype=Knet.atype()`: array and element type for parameter initialization\n",
    "* `init=ùëº(‚àö(6/(fanin+fanout)))`: initialization function for weights\n",
    "* `binit=zeros`: initialization function for bias, if `nothing` do not use bias\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: relu not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: relu not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[10]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091",
      " [3] execute_code(::String, ::String) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:27",
      " [4] execute_request(::ZMQ.Socket, ::IJulia.Msg) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:86",
      " [5] #invokelatest#1 at .\\essentials.jl:710 [inlined]",
      " [6] invokelatest at .\\essentials.jl:709 [inlined]",
      " [7] eventloop(::ZMQ.Socket) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\eventloop.jl:8",
      " [8] (::IJulia.var\"#15#18\")() at .\\task.jl:356"
     ]
    }
   ],
   "source": [
    "dense_layer = Dense(2, dim = 2, f = relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Layers\n",
    "To chain layers, one must declare a callable object that can iterate over the layers and output the final value or return the result of the cost function when (x,y) provided. Currently, Knet does not have a built-in model that is capable of these operations. An example is placed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Chain; layers; Chain(args...)= new(args);end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple struct that has a field layers which will hold the given layers as a tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callable object is created that takes an input and applies the logic given in the layers in the given order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c::Chain)(x,y) = nll(c(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callable object that returns the cost of a(x) and y (true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: relu not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: relu not defined",
      "",
      "Stacktrace:",
      " [1] Conv(::Int64, ::Int64, ::Int64, ::Int64) at .\\In[7]:3",
      " [2] top-level scope at In[14]:1",
      " [3] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091",
      " [4] execute_code(::String, ::String) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:27",
      " [5] execute_request(::ZMQ.Socket, ::IJulia.Msg) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:86",
      " [6] #invokelatest#1 at .\\essentials.jl:710 [inlined]",
      " [7] invokelatest at .\\essentials.jl:709 [inlined]",
      " [8] eventloop(::ZMQ.Socket) at C:\\Users\\PC\\.julia\\packages\\IJulia\\rWZ9e\\src\\eventloop.jl:8",
      " [9] (::IJulia.var\"#15#18\")() at .\\task.jl:356"
     ]
    }
   ],
   "source": [
    "LeNet = Chain(Conv(5,5,1,20), Conv(5,5,20,50), dense(800,500), dense(500,10,identity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
